{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dodiscover DoDiscover is a Python library for causal discovery (causal structure learning). If one does not have access to a hypothesized causal graph for their situation, then they may use dodiscover to learn causal structure from their data (e.g. in the form of a graph). Documentation See the development version documentation . Or see stable version documentation Installation Installation is best done via pip or conda . For developers, they can also install from source using pip . See installation page for full details. Dependencies Minimally, dodiscover requires: * Python (>=3.8) * numpy * scipy * networkx * pywhy-graphs User Installation If you already have a working installation of numpy, scipy and networkx, the easiest way to install dodiscover is using pip : # doesn 't work until we make an official release :p pip install -U dodiscover # If you are a developer and would like to install the developer dependencies pip install -e . To install the package from github, clone the repository and then cd into the directory: poetry install -- with docs # if you would like an editable install of dodiscover for dev purposes pip install - e .","title":"Home"},{"location":"#dodiscover","text":"DoDiscover is a Python library for causal discovery (causal structure learning). If one does not have access to a hypothesized causal graph for their situation, then they may use dodiscover to learn causal structure from their data (e.g. in the form of a graph).","title":"dodiscover"},{"location":"#documentation","text":"See the development version documentation . Or see stable version documentation","title":"Documentation"},{"location":"#installation","text":"Installation is best done via pip or conda . For developers, they can also install from source using pip . See installation page for full details.","title":"Installation"},{"location":"#dependencies","text":"Minimally, dodiscover requires: * Python (>=3.8) * numpy * scipy * networkx * pywhy-graphs","title":"Dependencies"},{"location":"#user-installation","text":"If you already have a working installation of numpy, scipy and networkx, the easiest way to install dodiscover is using pip : # doesn 't work until we make an official release :p pip install -U dodiscover # If you are a developer and would like to install the developer dependencies pip install -e . To install the package from github, clone the repository and then cd into the directory: poetry install -- with docs # if you would like an editable install of dodiscover for dev purposes pip install - e .","title":"User Installation"},{"location":"CONTRIBUTING/","text":"Contributing Thanks for considering contributing! Please read this document to learn the various ways you can contribute to this project and how to go about doing it. Bug reports and feature requests Did you find a bug? First, do a quick search to see whether your issue has already been reported. If your issue has already been reported, please comment on the existing issue. Otherwise, open a new GitHub issue . Be sure to include a clear title and description. The description should include as much relevant information as possible. The description should explain how to reproduce the erroneous behavior as well as the behavior you expect to see. Ideally you would include a code sample or an executable test case demonstrating the expected behavior. Do you have a suggestion for an enhancement or new feature? We use GitHub issues to track feature requests. Before you create an feature request: Make sure you have a clear idea of the enhancement you would like. If you have a vague idea, consider discussing it first on a GitHub issue. Check the documentation to make sure your feature does not already exist. Do a quick search to see whether your feature has already been suggested. When creating your request, please: Provide a clear title and description. Explain why the enhancement would be useful. It may be helpful to highlight the feature in other libraries. Include code examples to demonstrate how the enhancement would be used. Making a pull request When you're ready to contribute code to address an open issue, please follow these guidelines to help us be able to review your pull request (PR) quickly. Initial setup (only do this once) Expand details \ud83d\udc47 If you haven't already done so, please fork this repository on GitHub. Then clone your fork locally with git clone https://github.com/USERNAME/dodiscover.git or git clone git @github . com : USERNAME / dodiscover . git At this point the local clone of your fork only knows that it came from your repo, github.com/USERNAME/dodiscover.git, but doesn't know anything the main repo, https://github.com/py-why/dodiscover.git . You can see this by running # Note you should be in the \"dodiscover\" directory . If you 're not # run \"cd ./dodiscover\" to change directory into the repo git remote -v which will output something like this: origin https://github.com/USERNAME/dodiscover.git (fetch) origin https://github.com/USERNAME/dodiscover.git (push) This means that your local clone can only track changes from your fork, but not from the main repo, and so you won't be able to keep your fork up-to-date with the main repo over time. Therefore you'll need to add another \"remote\" to your clone that points to https://github.com/py-why/dodiscover.git . To do this, run the following: git remote add upstream https://github.com/py-why/dodiscover.git Now if you do git remote -v again, you'll see origin https://github.com/USERNAME/dodiscover.git (fetch) origin https://github.com/USERNAME/dodiscover.git (push) upstream https://github.com/py-why/dodiscover.git (fetch) upstream https://github.com/py-why/dodiscover.git (push) Finally, you'll need to create a Python 3 virtual environment suitable for working on this project. There a number of tools out there that making working with virtual environments easier. The most direct way is with the venv module in the standard library, but if you're new to Python or you don't already have a recent Python 3 version installed on your machine, we recommend Miniconda . On Mac, for example, you can install Miniconda with Homebrew : brew install miniconda Then you can create and activate a new Python environment by running: conda create -n dodiscover python=3.9 conda activate dodiscover Once your virtual environment is activated, you can install your local clone in \"editable mode\" with pip install - U pip setuptools wheel pip install - e . [ dev ] The \"editable mode\" comes from the -e argument to pip , and essential just creates a symbolic link from the site-packages directory of your virtual environment to the source code in your local clone. That way any changes you make will be immediately reflected in your virtual environment. Ensure your fork is up-to-date Expand details \ud83d\udc47 Once you've added an \"upstream\" remote pointing to https://github.com/allenai/python-package-temlate.git , keeping your fork up-to-date is easy: git checkout main # if not already on main git pull -- rebase upstream main git push Create a new branch to work on your fix or enhancement Expand details \ud83d\udc47 Committing directly to the main branch of your fork is not recommended. It will be easier to keep your fork clean if you work on a separate branch for each contribution you intend to make. You can create a new branch with # replace BRANCH with whatever name you want to give it git checkout -b BRANCH git push -u origin BRANCH Developing and testing your changes Expand details \ud83d\udc47 Our continuous integration (CI) testing runs a number of checks for each pull request on GitHub Actions . You can run most of these tests locally, which is something you should do before opening a PR to help speed up the review process and make it easier for us. Please see our development guide for a comprehensive overview of useful commands leveraging poetry . This will cover aspects of code style checking, unit testing, integration testing, and building the documentation. We try to make it as easy as possible with copy/paste commands leveraging poetry which will guide your development process! And finally, please update the CHANGELOG with notes on your contribution in the \"Unreleased\" section at the top. After all of the above checks have passed, you can now open a new GitHub pull request . Make sure you have a clear description of the problem and the solution, and include a link to relevant issues. We look forward to reviewing your PR! Writing docstrings We use Sphinx to build our API docs, which automatically parses all docstrings of public classes and methods. All docstrings should adhere to the Numpy styling convention .","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Thanks for considering contributing! Please read this document to learn the various ways you can contribute to this project and how to go about doing it.","title":"Contributing"},{"location":"CONTRIBUTING/#bug-reports-and-feature-requests","text":"","title":"Bug reports and feature requests"},{"location":"CONTRIBUTING/#did-you-find-a-bug","text":"First, do a quick search to see whether your issue has already been reported. If your issue has already been reported, please comment on the existing issue. Otherwise, open a new GitHub issue . Be sure to include a clear title and description. The description should include as much relevant information as possible. The description should explain how to reproduce the erroneous behavior as well as the behavior you expect to see. Ideally you would include a code sample or an executable test case demonstrating the expected behavior.","title":"Did you find a bug?"},{"location":"CONTRIBUTING/#do-you-have-a-suggestion-for-an-enhancement-or-new-feature","text":"We use GitHub issues to track feature requests. Before you create an feature request: Make sure you have a clear idea of the enhancement you would like. If you have a vague idea, consider discussing it first on a GitHub issue. Check the documentation to make sure your feature does not already exist. Do a quick search to see whether your feature has already been suggested. When creating your request, please: Provide a clear title and description. Explain why the enhancement would be useful. It may be helpful to highlight the feature in other libraries. Include code examples to demonstrate how the enhancement would be used.","title":"Do you have a suggestion for an enhancement or new feature?"},{"location":"CONTRIBUTING/#making-a-pull-request","text":"When you're ready to contribute code to address an open issue, please follow these guidelines to help us be able to review your pull request (PR) quickly. Initial setup (only do this once) Expand details \ud83d\udc47 If you haven't already done so, please fork this repository on GitHub. Then clone your fork locally with git clone https://github.com/USERNAME/dodiscover.git or git clone git @github . com : USERNAME / dodiscover . git At this point the local clone of your fork only knows that it came from your repo, github.com/USERNAME/dodiscover.git, but doesn't know anything the main repo, https://github.com/py-why/dodiscover.git . You can see this by running # Note you should be in the \"dodiscover\" directory . If you 're not # run \"cd ./dodiscover\" to change directory into the repo git remote -v which will output something like this: origin https://github.com/USERNAME/dodiscover.git (fetch) origin https://github.com/USERNAME/dodiscover.git (push) This means that your local clone can only track changes from your fork, but not from the main repo, and so you won't be able to keep your fork up-to-date with the main repo over time. Therefore you'll need to add another \"remote\" to your clone that points to https://github.com/py-why/dodiscover.git . To do this, run the following: git remote add upstream https://github.com/py-why/dodiscover.git Now if you do git remote -v again, you'll see origin https://github.com/USERNAME/dodiscover.git (fetch) origin https://github.com/USERNAME/dodiscover.git (push) upstream https://github.com/py-why/dodiscover.git (fetch) upstream https://github.com/py-why/dodiscover.git (push) Finally, you'll need to create a Python 3 virtual environment suitable for working on this project. There a number of tools out there that making working with virtual environments easier. The most direct way is with the venv module in the standard library, but if you're new to Python or you don't already have a recent Python 3 version installed on your machine, we recommend Miniconda . On Mac, for example, you can install Miniconda with Homebrew : brew install miniconda Then you can create and activate a new Python environment by running: conda create -n dodiscover python=3.9 conda activate dodiscover Once your virtual environment is activated, you can install your local clone in \"editable mode\" with pip install - U pip setuptools wheel pip install - e . [ dev ] The \"editable mode\" comes from the -e argument to pip , and essential just creates a symbolic link from the site-packages directory of your virtual environment to the source code in your local clone. That way any changes you make will be immediately reflected in your virtual environment. Ensure your fork is up-to-date Expand details \ud83d\udc47 Once you've added an \"upstream\" remote pointing to https://github.com/allenai/python-package-temlate.git , keeping your fork up-to-date is easy: git checkout main # if not already on main git pull -- rebase upstream main git push Create a new branch to work on your fix or enhancement Expand details \ud83d\udc47 Committing directly to the main branch of your fork is not recommended. It will be easier to keep your fork clean if you work on a separate branch for each contribution you intend to make. You can create a new branch with # replace BRANCH with whatever name you want to give it git checkout -b BRANCH git push -u origin BRANCH Developing and testing your changes Expand details \ud83d\udc47 Our continuous integration (CI) testing runs a number of checks for each pull request on GitHub Actions . You can run most of these tests locally, which is something you should do before opening a PR to help speed up the review process and make it easier for us. Please see our development guide for a comprehensive overview of useful commands leveraging poetry . This will cover aspects of code style checking, unit testing, integration testing, and building the documentation. We try to make it as easy as possible with copy/paste commands leveraging poetry which will guide your development process! And finally, please update the CHANGELOG with notes on your contribution in the \"Unreleased\" section at the top. After all of the above checks have passed, you can now open a new GitHub pull request . Make sure you have a clear description of the problem and the solution, and include a link to relevant issues. We look forward to reviewing your PR!","title":"Making a pull request"},{"location":"CONTRIBUTING/#writing-docstrings","text":"We use Sphinx to build our API docs, which automatically parses all docstrings of public classes and methods. All docstrings should adhere to the Numpy styling convention .","title":"Writing docstrings"},{"location":"DEVELOPING/","text":"Requirements Python 3.8+ Poetry ( curl -sSL https://install.python-poetry.org | python ) For the other requirements, inspect the pyproject.toml file. If you are updated the dependencies, please run poetry update to update the Development Tasks There are a series of top-level tasks available through Poetry. These can each be run via poetry run poe <taskname> Basic Verification apply_format - runs the suite of formatting tools applying tools to make code compliant check_format - runs the suite of formatting tools checking for compliance lint - runs the suite of linting tools unit_test - executes fast unit tests typecheck - performs static typechecking of the codebase using mypy verify - executes the basic PR verification suite, which includes all the tasks listed above Longer Verification integration_test - runs slower tests and end-to-end tests are run through this task Docsite start_docs - start the API documentation site locally build_docs - build the API documentation site For convenience you can run a check for all style components necessary: make run - checks This will run isort , black , flake8 , mypy , check - manifest , and pydocstyle on the entire repository . Please fix your errors if you see any . First , you should run [ ` isort ` ]( https : //github.com/PyCQA/isort) and [`black`](https://github.com/psf/black) to make sure you code is formatted consistently. Many IDEs support code formatters as plugins , so you may be able to setup isort and black to run automatically every time you save . For example , [ ` black . vim ` ]( https : //github.com/psf/black/tree/master/plugin) will give you this functionality in Vim. But both `isort` and `black` are also easy to run directly from the command line. Just run this from the root of your clone : isort . black . Our CI also uses [ ` flake8 ` ]( https : //github.com/py-why/dodiscover/tree/main/tests) to lint the code base and [`mypy`](http://mypy-lang.org/) for type-checking. You should run both of these next with flake8 . and mypy . We also strive to maintain high test coverage , so most contributions should include additions to [ the unit tests ]( https : //github.com/py-why/dodiscover/tree/main/tests). These tests are run with [`pytest`](https://docs.pytest.org/en/latest/), which you can use to locally run any test modules that you've added or changed. For example , if you ' ve fixed a bug in ` mne_icalabel / a / b . py ` , you can run the tests specific to that module with pytest - v tests / a / b_test . py Our CI will automatically check that test coverage stays above a certain threshold ( around 90 % ). To check the coverage locally in this example , you could run pytest - v -- cov mne_icalabel . a . b tests / a / b_test . py If your contribution involves additions to any public part of the API , we require that you write docstrings for each function , method , class , or module that you add . See the [ Writing docstrings ]( # writing - docstrings ) section below for details on the syntax . You should test to make sure the API documentation can build without errors by running cd doc make html If the build fails , it ' s most likely due to small formatting issues . If the error message isn ' t clear , feel free to comment on this in your pull request .","title":"Developing"},{"location":"DEVELOPING/#requirements","text":"Python 3.8+ Poetry ( curl -sSL https://install.python-poetry.org | python ) For the other requirements, inspect the pyproject.toml file. If you are updated the dependencies, please run poetry update to update the","title":"Requirements"},{"location":"DEVELOPING/#development-tasks","text":"There are a series of top-level tasks available through Poetry. These can each be run via poetry run poe <taskname>","title":"Development Tasks"},{"location":"DEVELOPING/#basic-verification","text":"apply_format - runs the suite of formatting tools applying tools to make code compliant check_format - runs the suite of formatting tools checking for compliance lint - runs the suite of linting tools unit_test - executes fast unit tests typecheck - performs static typechecking of the codebase using mypy verify - executes the basic PR verification suite, which includes all the tasks listed above","title":"Basic Verification"},{"location":"DEVELOPING/#longer-verification","text":"integration_test - runs slower tests and end-to-end tests are run through this task","title":"Longer Verification"},{"location":"DEVELOPING/#docsite","text":"start_docs - start the API documentation site locally build_docs - build the API documentation site For convenience you can run a check for all style components necessary: make run - checks This will run isort , black , flake8 , mypy , check - manifest , and pydocstyle on the entire repository . Please fix your errors if you see any . First , you should run [ ` isort ` ]( https : //github.com/PyCQA/isort) and [`black`](https://github.com/psf/black) to make sure you code is formatted consistently. Many IDEs support code formatters as plugins , so you may be able to setup isort and black to run automatically every time you save . For example , [ ` black . vim ` ]( https : //github.com/psf/black/tree/master/plugin) will give you this functionality in Vim. But both `isort` and `black` are also easy to run directly from the command line. Just run this from the root of your clone : isort . black . Our CI also uses [ ` flake8 ` ]( https : //github.com/py-why/dodiscover/tree/main/tests) to lint the code base and [`mypy`](http://mypy-lang.org/) for type-checking. You should run both of these next with flake8 . and mypy . We also strive to maintain high test coverage , so most contributions should include additions to [ the unit tests ]( https : //github.com/py-why/dodiscover/tree/main/tests). These tests are run with [`pytest`](https://docs.pytest.org/en/latest/), which you can use to locally run any test modules that you've added or changed. For example , if you ' ve fixed a bug in ` mne_icalabel / a / b . py ` , you can run the tests specific to that module with pytest - v tests / a / b_test . py Our CI will automatically check that test coverage stays above a certain threshold ( around 90 % ). To check the coverage locally in this example , you could run pytest - v -- cov mne_icalabel . a . b tests / a / b_test . py If your contribution involves additions to any public part of the API , we require that you write docstrings for each function , method , class , or module that you add . See the [ Writing docstrings ]( # writing - docstrings ) section below for details on the syntax . You should test to make sure the API documentation can build without errors by running cd doc make html If the build fails , it ' s most likely due to small formatting issues . If the error message isn ' t clear , feel free to comment on this in your pull request .","title":"Docsite"},{"location":"reference/dodiscover/","text":"Module dodiscover DoDiscover - a library for Python-based Causal Discovery None View Source \"\"\" DoDiscover - a library for Python-based Causal Discovery \"\"\" from ._version import __version__ # noqa: F401 Sub-modules dodiscover.ci","title":"Index"},{"location":"reference/dodiscover/#module-dodiscover","text":"DoDiscover - a library for Python-based Causal Discovery None View Source \"\"\" DoDiscover - a library for Python-based Causal Discovery \"\"\" from ._version import __version__ # noqa: F401","title":"Module dodiscover"},{"location":"reference/dodiscover/#sub-modules","text":"dodiscover.ci","title":"Sub-modules"},{"location":"reference/dodiscover/ci/","text":"Module dodiscover.ci None None View Source from .base import BaseConditionalIndependenceTest from .fisher_z_test import FisherZCITest from .g_test import GSquareCITest from .kernel_test import KernelCITest from .oracle import Oracle , ParentChildOracle Sub-modules dodiscover.ci.base dodiscover.ci.fisher_z_test dodiscover.ci.g_test dodiscover.ci.kernel_test dodiscover.ci.oracle","title":"Index"},{"location":"reference/dodiscover/ci/#module-dodiscoverci","text":"None None View Source from .base import BaseConditionalIndependenceTest from .fisher_z_test import FisherZCITest from .g_test import GSquareCITest from .kernel_test import KernelCITest from .oracle import Oracle , ParentChildOracle","title":"Module dodiscover.ci"},{"location":"reference/dodiscover/ci/#sub-modules","text":"dodiscover.ci.base dodiscover.ci.fisher_z_test dodiscover.ci.g_test dodiscover.ci.kernel_test dodiscover.ci.oracle","title":"Sub-modules"},{"location":"reference/dodiscover/ci/base/","text":"Module dodiscover.ci.base None None View Source from abc import ABCMeta , abstractmethod from typing import Any , Tuple import pandas as pd class BaseConditionalIndependenceTest ( metaclass = ABCMeta ): \"\"\"Abstract class for any conditional independence test. All CI tests are used in constraint-based causal discovery algorithms. This class interface is expected to be very lightweight to enable anyone to convert a function for CI testing into a class, which has a specific API. \"\"\" def _check_test_input ( self , df : pd . DataFrame , x_var , y_var , z_covariates ): if any ( col not in df . columns for col in [ x_var , y_var ]): raise ValueError ( \"The x and y variables are not both in the DataFrame.\" ) if z_covariates is not None and any ( col not in df . columns for col in z_covariates ): raise ValueError ( \"The z conditioning set variables are not all in the DataFrame.\" ) @abstractmethod def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Abstract method for all conditional independence tests. Parameters ---------- df : pd.DataFrame _description_ x_var : Any _description_ y_var : Any _description_ z_covariates : Any, optional _description_, by default None Returns ------- Tuple[float, float] _description_ \"\"\" pass Classes BaseConditionalIndependenceTest class BaseConditionalIndependenceTest ( / , * args , ** kwargs ) View Source class BaseConditionalIndependenceTest ( metaclass = ABCMeta ): \"\"\"Abstract class for any conditional independence test. All CI tests are used in constraint-based causal discovery algorithms. This class interface is expected to be very lightweight to enable anyone to convert a function for CI testing into a class, which has a specific API. \"\"\" def _check_test_input ( self , df : pd . DataFrame , x_var , y_var , z_covariates ): if any ( col not in df . columns for col in [ x_var , y_var ]): raise ValueError ( \"The x and y variables are not both in the DataFrame.\" ) if z_covariates is not None and any ( col not in df . columns for col in z_covariates ): raise ValueError ( \"The z conditioning set variables are not all in the DataFrame.\" ) @ abstractmethod def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Abstract method for all conditional independence tests. Parameters ---------- df : pd.DataFrame _description_ x_var : Any _description_ y_var : Any _description_ z_covariates : Any, optional _description_, by default None Returns ------- Tuple[float, float] _description_ \"\"\" pass Descendants dodiscover.ci.fisher_z_test.FisherZCITest dodiscover.ci.g_test.GSquareCITest dodiscover.ci.kernel_test.KernelCITest dodiscover.ci.oracle.Oracle Methods test def test ( self , df : pandas . core . frame . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ] Abstract method for all conditional independence tests. Parameters: Name Type Description Default df pd.DataFrame description None x_var Any description None y_var Any description None z_covariates Any description , by default None None Returns: Type Description Tuple[float, float] description View Source @ abstractmethod def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Abstract method for all conditional independence tests. Parameters ---------- df : pd.DataFrame _description_ x_var : Any _description_ y_var : Any _description_ z_covariates : Any, optional _description_, by default None Returns ------- Tuple[float, float] _description_ \"\"\" pass","title":"Base"},{"location":"reference/dodiscover/ci/base/#module-dodiscovercibase","text":"None None View Source from abc import ABCMeta , abstractmethod from typing import Any , Tuple import pandas as pd class BaseConditionalIndependenceTest ( metaclass = ABCMeta ): \"\"\"Abstract class for any conditional independence test. All CI tests are used in constraint-based causal discovery algorithms. This class interface is expected to be very lightweight to enable anyone to convert a function for CI testing into a class, which has a specific API. \"\"\" def _check_test_input ( self , df : pd . DataFrame , x_var , y_var , z_covariates ): if any ( col not in df . columns for col in [ x_var , y_var ]): raise ValueError ( \"The x and y variables are not both in the DataFrame.\" ) if z_covariates is not None and any ( col not in df . columns for col in z_covariates ): raise ValueError ( \"The z conditioning set variables are not all in the DataFrame.\" ) @abstractmethod def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Abstract method for all conditional independence tests. Parameters ---------- df : pd.DataFrame _description_ x_var : Any _description_ y_var : Any _description_ z_covariates : Any, optional _description_, by default None Returns ------- Tuple[float, float] _description_ \"\"\" pass","title":"Module dodiscover.ci.base"},{"location":"reference/dodiscover/ci/base/#classes","text":"","title":"Classes"},{"location":"reference/dodiscover/ci/base/#baseconditionalindependencetest","text":"class BaseConditionalIndependenceTest ( / , * args , ** kwargs ) View Source class BaseConditionalIndependenceTest ( metaclass = ABCMeta ): \"\"\"Abstract class for any conditional independence test. All CI tests are used in constraint-based causal discovery algorithms. This class interface is expected to be very lightweight to enable anyone to convert a function for CI testing into a class, which has a specific API. \"\"\" def _check_test_input ( self , df : pd . DataFrame , x_var , y_var , z_covariates ): if any ( col not in df . columns for col in [ x_var , y_var ]): raise ValueError ( \"The x and y variables are not both in the DataFrame.\" ) if z_covariates is not None and any ( col not in df . columns for col in z_covariates ): raise ValueError ( \"The z conditioning set variables are not all in the DataFrame.\" ) @ abstractmethod def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Abstract method for all conditional independence tests. Parameters ---------- df : pd.DataFrame _description_ x_var : Any _description_ y_var : Any _description_ z_covariates : Any, optional _description_, by default None Returns ------- Tuple[float, float] _description_ \"\"\" pass","title":"BaseConditionalIndependenceTest"},{"location":"reference/dodiscover/ci/base/#descendants","text":"dodiscover.ci.fisher_z_test.FisherZCITest dodiscover.ci.g_test.GSquareCITest dodiscover.ci.kernel_test.KernelCITest dodiscover.ci.oracle.Oracle","title":"Descendants"},{"location":"reference/dodiscover/ci/base/#methods","text":"","title":"Methods"},{"location":"reference/dodiscover/ci/base/#test","text":"def test ( self , df : pandas . core . frame . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ] Abstract method for all conditional independence tests. Parameters: Name Type Description Default df pd.DataFrame description None x_var Any description None y_var Any description None z_covariates Any description , by default None None Returns: Type Description Tuple[float, float] description View Source @ abstractmethod def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Abstract method for all conditional independence tests. Parameters ---------- df : pd.DataFrame _description_ x_var : Any _description_ y_var : Any _description_ z_covariates : Any, optional _description_, by default None Returns ------- Tuple[float, float] _description_ \"\"\" pass","title":"test"},{"location":"reference/dodiscover/ci/fisher_z_test/","text":"Module dodiscover.ci.fisher_z_test None None View Source from math import log , sqrt from typing import Any , Set , Tuple , Union import numpy as np import pandas as pd from scipy.stats import norm from .base import BaseConditionalIndependenceTest class FisherZCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , correlation_matrix = None ): \"\"\"Conditional independence test using Fisher-Z's test for Gaussian random variables. Parameters ---------- correlation_matrix : np.ndarray of shape (n_variables, n_variables), optional ``None`` means without the parameter of correlation matrix and the correlation will be computed from the data., by default None \"\"\" self . correlation_matrix = correlation_matrix def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Run conditional independence test. Parameters ---------- df : pd.DataFrame _description_ x : Any _description_ y : Any _description_ z : Any, optional _description_, by default None Returns ------- stat : float The test statistic. pvalue : float The p-value of the test. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () stat , pvalue = fisherz ( df , x_var , y_var , z_covariates , self . correlation_matrix ) return stat , pvalue def fisherz ( data : pd . DataFrame , x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , correlation_matrix = None , ): \"\"\"Perform an independence test using Fisher-Z's test. Works on Gaussian random variables. Parameters ---------- data : pd.DataFrame The data. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). correlation_matrix : np.ndarray of shape (n_variables, n_variables), optional ``None`` means without the parameter of correlation matrix and the correlation will be computed from the data., by default None Returns ------- X : float The test statistic. p : float The p-value of the test. \"\"\" data_arr = data . to_numpy () if correlation_matrix is None : correlation_matrix = np . corrcoef ( data_arr . T ) sample_size = data . shape [ 0 ] var = list ({ x , y } . union ( sep_set )) # type: ignore ( var_idx ,) = np . in1d ( data . columns , var ) . nonzero () # compute the correlation matrix within the specified data sub_corr_matrix = correlation_matrix [ np . ix_ ( var_idx , var_idx )] # try: inv = np . linalg . inv ( sub_corr_matrix ) r = - inv [ 0 , 1 ] / sqrt ( inv [ 0 , 0 ] * inv [ 1 , 1 ]) # except Exception as e: # print('inside errors...') # print(var) # print(correlation_matrix.shape) # print(sub_corr_matrix) # print(sub_corr_matrix.shape) # print(var_idx) # raise Exception(e) # apply the Fisher Z-transformation Z = 0.5 * log (( 1 + r ) / ( 1 - r )) # compute the test statistic X = sqrt ( sample_size - len ( sep_set ) - 3 ) * abs ( Z ) p = 2 * ( 1 - norm . cdf ( abs ( X ))) return X , p Functions fisherz def fisherz ( data : pandas . core . frame . DataFrame , x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , correlation_matrix = None ) Perform an independence test using Fisher-Z's test. Works on Gaussian random variables. Parameters: Name Type Description Default data pd.DataFrame The data. None x int str the first node variable. If data is a DataFrame, then 'x' must be in the columns of data . None y int str the second node variable. If data is a DataFrame, then 'y' must be in the columns of data . None sep_set set the set of neibouring nodes of x and y (as a set()). None correlation_matrix np.ndarray of shape (n_variables, n_variables) None means without the parameter of correlation matrix and the correlation will be computed from the data., by default None None Returns: Type Description float The test statistic. View Source def fisherz ( data : pd . DataFrame , x : Union [ int , str ] , y : Union [ int , str ] , sep_set : Set , correlation_matrix = None , ) : \" \"\" Perform an independence test using Fisher-Z's test. Works on Gaussian random variables. Parameters ---------- data : pd.DataFrame The data. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). correlation_matrix : np.ndarray of shape (n_variables, n_variables), optional ``None`` means without the parameter of correlation matrix and the correlation will be computed from the data., by default None Returns ------- X : float The test statistic. p : float The p-value of the test. \"\" \" data_arr = data . to_numpy () if correlation_matrix is None : correlation_matrix = np . corrcoef ( data_arr . T ) sample_size = data . shape [ 0 ] var = list ( { x , y } . union ( sep_set )) # type: ignore ( var_idx ,) = np . in1d ( data . columns , var ). nonzero () # compute the correlation matrix within the specified data sub_corr_matrix = correlation_matrix [ np . ix_ ( var_idx , var_idx ) ] # try: inv = np . linalg . inv ( sub_corr_matrix ) r = - inv [ 0 , 1 ] / sqrt ( inv [ 0 , 0 ] * inv [ 1 , 1 ] ) # except Exception as e: # print('inside errors...') # print(var) # print(correlation_matrix.shape) # print(sub_corr_matrix) # print(sub_corr_matrix.shape) # print(var_idx) # raise Exception(e) # apply the Fisher Z-transformation Z = 0.5 * log (( 1 + r ) / ( 1 - r )) # compute the test statistic X = sqrt ( sample_size - len ( sep_set ) - 3 ) * abs ( Z ) p = 2 * ( 1 - norm . cdf ( abs ( X ))) return X , p Classes FisherZCITest class FisherZCITest ( correlation_matrix = None ) View Source class FisherZCITest ( BaseConditionalIndependenceTest ) : def __init__ ( self , correlation_matrix = None ) : \" \"\" Conditional independence test using Fisher-Z's test for Gaussian random variables. Parameters ---------- correlation_matrix : np.ndarray of shape (n_variables, n_variables), optional ``None`` means without the parameter of correlation matrix and the correlation will be computed from the data., by default None \"\" \" self . correlation_matrix = correlation_matrix def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ] : \" \"\" Run conditional independence test. Parameters ---------- df : pd.DataFrame _description_ x : Any _description_ y : Any _description_ z : Any, optional _description_, by default None Returns ------- stat : float The test statistic. pvalue : float The p-value of the test. \"\" \" self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () stat , pvalue = fisherz ( df , x_var , y_var , z_covariates , self . correlation_matrix ) return stat , pvalue Ancestors (in MRO) dodiscover.ci.base.BaseConditionalIndependenceTest Methods test def test ( self , df : pandas . core . frame . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ] Run conditional independence test. Parameters: Name Type Description Default df pd.DataFrame description None x Any description None y Any description None z Any description , by default None None Returns: Type Description float The test statistic. View Source def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Run conditional independence test. Parameters ---------- df : pd.DataFrame _description_ x : Any _description_ y : Any _description_ z : Any, optional _description_, by default None Returns ------- stat : float The test statistic. pvalue : float The p-value of the test. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () stat , pvalue = fisherz ( df , x_var , y_var , z_covariates , self . correlation_matrix ) return stat , pvalue","title":"Fisher Z Test"},{"location":"reference/dodiscover/ci/fisher_z_test/#module-dodiscovercifisher_z_test","text":"None None View Source from math import log , sqrt from typing import Any , Set , Tuple , Union import numpy as np import pandas as pd from scipy.stats import norm from .base import BaseConditionalIndependenceTest class FisherZCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , correlation_matrix = None ): \"\"\"Conditional independence test using Fisher-Z's test for Gaussian random variables. Parameters ---------- correlation_matrix : np.ndarray of shape (n_variables, n_variables), optional ``None`` means without the parameter of correlation matrix and the correlation will be computed from the data., by default None \"\"\" self . correlation_matrix = correlation_matrix def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Run conditional independence test. Parameters ---------- df : pd.DataFrame _description_ x : Any _description_ y : Any _description_ z : Any, optional _description_, by default None Returns ------- stat : float The test statistic. pvalue : float The p-value of the test. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () stat , pvalue = fisherz ( df , x_var , y_var , z_covariates , self . correlation_matrix ) return stat , pvalue def fisherz ( data : pd . DataFrame , x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , correlation_matrix = None , ): \"\"\"Perform an independence test using Fisher-Z's test. Works on Gaussian random variables. Parameters ---------- data : pd.DataFrame The data. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). correlation_matrix : np.ndarray of shape (n_variables, n_variables), optional ``None`` means without the parameter of correlation matrix and the correlation will be computed from the data., by default None Returns ------- X : float The test statistic. p : float The p-value of the test. \"\"\" data_arr = data . to_numpy () if correlation_matrix is None : correlation_matrix = np . corrcoef ( data_arr . T ) sample_size = data . shape [ 0 ] var = list ({ x , y } . union ( sep_set )) # type: ignore ( var_idx ,) = np . in1d ( data . columns , var ) . nonzero () # compute the correlation matrix within the specified data sub_corr_matrix = correlation_matrix [ np . ix_ ( var_idx , var_idx )] # try: inv = np . linalg . inv ( sub_corr_matrix ) r = - inv [ 0 , 1 ] / sqrt ( inv [ 0 , 0 ] * inv [ 1 , 1 ]) # except Exception as e: # print('inside errors...') # print(var) # print(correlation_matrix.shape) # print(sub_corr_matrix) # print(sub_corr_matrix.shape) # print(var_idx) # raise Exception(e) # apply the Fisher Z-transformation Z = 0.5 * log (( 1 + r ) / ( 1 - r )) # compute the test statistic X = sqrt ( sample_size - len ( sep_set ) - 3 ) * abs ( Z ) p = 2 * ( 1 - norm . cdf ( abs ( X ))) return X , p","title":"Module dodiscover.ci.fisher_z_test"},{"location":"reference/dodiscover/ci/fisher_z_test/#functions","text":"","title":"Functions"},{"location":"reference/dodiscover/ci/fisher_z_test/#fisherz","text":"def fisherz ( data : pandas . core . frame . DataFrame , x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , correlation_matrix = None ) Perform an independence test using Fisher-Z's test. Works on Gaussian random variables. Parameters: Name Type Description Default data pd.DataFrame The data. None x int str the first node variable. If data is a DataFrame, then 'x' must be in the columns of data . None y int str the second node variable. If data is a DataFrame, then 'y' must be in the columns of data . None sep_set set the set of neibouring nodes of x and y (as a set()). None correlation_matrix np.ndarray of shape (n_variables, n_variables) None means without the parameter of correlation matrix and the correlation will be computed from the data., by default None None Returns: Type Description float The test statistic. View Source def fisherz ( data : pd . DataFrame , x : Union [ int , str ] , y : Union [ int , str ] , sep_set : Set , correlation_matrix = None , ) : \" \"\" Perform an independence test using Fisher-Z's test. Works on Gaussian random variables. Parameters ---------- data : pd.DataFrame The data. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). correlation_matrix : np.ndarray of shape (n_variables, n_variables), optional ``None`` means without the parameter of correlation matrix and the correlation will be computed from the data., by default None Returns ------- X : float The test statistic. p : float The p-value of the test. \"\" \" data_arr = data . to_numpy () if correlation_matrix is None : correlation_matrix = np . corrcoef ( data_arr . T ) sample_size = data . shape [ 0 ] var = list ( { x , y } . union ( sep_set )) # type: ignore ( var_idx ,) = np . in1d ( data . columns , var ). nonzero () # compute the correlation matrix within the specified data sub_corr_matrix = correlation_matrix [ np . ix_ ( var_idx , var_idx ) ] # try: inv = np . linalg . inv ( sub_corr_matrix ) r = - inv [ 0 , 1 ] / sqrt ( inv [ 0 , 0 ] * inv [ 1 , 1 ] ) # except Exception as e: # print('inside errors...') # print(var) # print(correlation_matrix.shape) # print(sub_corr_matrix) # print(sub_corr_matrix.shape) # print(var_idx) # raise Exception(e) # apply the Fisher Z-transformation Z = 0.5 * log (( 1 + r ) / ( 1 - r )) # compute the test statistic X = sqrt ( sample_size - len ( sep_set ) - 3 ) * abs ( Z ) p = 2 * ( 1 - norm . cdf ( abs ( X ))) return X , p","title":"fisherz"},{"location":"reference/dodiscover/ci/fisher_z_test/#classes","text":"","title":"Classes"},{"location":"reference/dodiscover/ci/fisher_z_test/#fisherzcitest","text":"class FisherZCITest ( correlation_matrix = None ) View Source class FisherZCITest ( BaseConditionalIndependenceTest ) : def __init__ ( self , correlation_matrix = None ) : \" \"\" Conditional independence test using Fisher-Z's test for Gaussian random variables. Parameters ---------- correlation_matrix : np.ndarray of shape (n_variables, n_variables), optional ``None`` means without the parameter of correlation matrix and the correlation will be computed from the data., by default None \"\" \" self . correlation_matrix = correlation_matrix def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ] : \" \"\" Run conditional independence test. Parameters ---------- df : pd.DataFrame _description_ x : Any _description_ y : Any _description_ z : Any, optional _description_, by default None Returns ------- stat : float The test statistic. pvalue : float The p-value of the test. \"\" \" self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () stat , pvalue = fisherz ( df , x_var , y_var , z_covariates , self . correlation_matrix ) return stat , pvalue","title":"FisherZCITest"},{"location":"reference/dodiscover/ci/fisher_z_test/#ancestors-in-mro","text":"dodiscover.ci.base.BaseConditionalIndependenceTest","title":"Ancestors (in MRO)"},{"location":"reference/dodiscover/ci/fisher_z_test/#methods","text":"","title":"Methods"},{"location":"reference/dodiscover/ci/fisher_z_test/#test","text":"def test ( self , df : pandas . core . frame . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ] Run conditional independence test. Parameters: Name Type Description Default df pd.DataFrame description None x Any description None y Any description None z Any description , by default None None Returns: Type Description float The test statistic. View Source def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None ) -> Tuple [ float , float ]: \"\"\"Run conditional independence test. Parameters ---------- df : pd.DataFrame _description_ x : Any _description_ y : Any _description_ z : Any, optional _description_, by default None Returns ------- stat : float The test statistic. pvalue : float The p-value of the test. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () stat , pvalue = fisherz ( df , x_var , y_var , z_covariates , self . correlation_matrix ) return stat , pvalue","title":"test"},{"location":"reference/dodiscover/ci/g_test/","text":"Module dodiscover.ci.g_test None None View Source # This code was originally adapted from https://github.com/keiichishima/gsq # and heavily refactored and modified. from typing import Any , List , Set , Tuple , Union import numpy as np import pandas as pd from numpy.typing import NDArray from scipy.stats import chi2 from .base import BaseConditionalIndependenceTest def _calculate_contingency_tble ( x : Union [ int , str ], y : Union [ int , str ], sep_set : Union [ List , Set ], dof : int , data : NDArray , nlevels_x : int , nlevels_y : int , levels : NDArray = None , ) -> NDArray : \"\"\"Calculate log term for binary G^2 statistic in CI test. Computes the contingency table and the associated log-term within the G^2 statistic for binary data. Parameters ---------- x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). dof : int The degrees of freedom. data : np.ndarray of shape (n_samples, n_variables) The input data matrix. nlevels_x : int The number of levels in the 'x' data variable. nlevels_y : int The number of levels in the 'y' data variable. levels : np.ndarray of shape (n_variables,) The number of levels associated with each variable. Returns ------- contingency_tble : np.ndarray of shape (2, 2, dof) A contingency table per degree of freedom. \"\"\" # define contingency table as a 2 by 2 table relating 'x' and 'y' # across different separating set variables contingency_tble = np . zeros (( nlevels_x , nlevels_y , dof )) x_idx = data [ x ] # [:, x] y_idx = data [ y ] # [:, y] sep_set = list ( sep_set ) # sum all co-occurrences of x and y conditioned on z for row_idx , ( idx , jdx ) in enumerate ( zip ( x_idx , y_idx )): kdx = 0 for zidx , z in enumerate ( sep_set ): if levels is None : # binary case kdx += data [ z ][ row_idx ] * int ( pow ( 2 , zidx )) else : # discrete case if zidx == 0 : kdx += data [ z ][ row_idx ] # data[row_idx, z] else : lprod = np . prod ( list ( map ( lambda x : levels [ x ], sep_set [: zidx ]))) # type: ignore kdx += data [ z ][ row_idx ] * lprod # increment the co-occurrence found contingency_tble [ idx , jdx , kdx ] += 1 return contingency_tble def _calculate_highdim_contingency ( x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , data : NDArray , nlevel_x : int , nlevels_y : int , ) -> NDArray : \"\"\"Calculate the contingency table for \"large\" separating set. When separating set is deemed \"large\", we use a different approach to computing the overall contingency table. Parameters ---------- x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). data : np.ndarray of shape (n_samples, n_variables) The input data matrix. nlevel_x : int Number of levels of the 'x' variable in the data matrix. nlevels_y : int Number of levels of the 'y' variable in the data matrix. Returns ------- contingency_tble : np.ndarray of shape (nlevel_x, nlevel_y, dof) A contingency table per degree of freedom per level of each variable. \"\"\" n_samples , _ = data . shape # keep track of all variables in the separating set sep_set = list ( sep_set ) # type: ignore k = data [:, sep_set ] # count number of value combinations for sepset variables # observed in the data dof_count = 1 parents_val = np . array ([ k [ 0 , :]]) # initialize the contingency table contingency_tble = np . zeros (( 2 , 2 , 1 )) xdx = data [ 0 , x ] ydx = data [ 0 , y ] contingency_tble [ xdx , ydx , dof_count - 1 ] = 1 # check how many parents we can create from the rest of the dataset for idx in range ( 1 , n_samples ): is_new = True xdx = data [ idx , x ] ydx = data [ idx , y ] # comparing the current values of the subset variables to all # already existing combinations of subset variables values tcomp = parents_val [: dof_count , :] == k [ idx , :] for it_parents in range ( dof_count ): if np . all ( tcomp [ it_parents , :]): contingency_tble [ xdx , ydx , it_parents ] += 1 is_new = False break # new combination of separating set values, so we create a new # contingency table if is_new : dof_count += 1 parents_val = np . r_ [ parents_val , [ k [ idx , :]]] # create a new contingnecy table and update cell counts # using the original table up to the last value ncontingency_tble = np . zeros (( nlevel_x , nlevels_y , dof_count )) for p in range ( dof_count - 1 ): ncontingency_tble [:, :, p ] = contingency_tble [:, :, p ] ncontingency_tble [ xdx , ydx , dof_count - 1 ] = 1 contingency_tble = ncontingency_tble return contingency_tble def _calculate_g_statistic ( contingency_tble ): \"\"\"Calculate a G statistic from contingency table. Parameters ---------- contingency_tble : np.ndarray of shape (nlevels_x, nlevels_y, dof) The contingency table of 'x' vs 'y'. Returns ------- G2 : float G^2 test statistic. \"\"\" nlevels_x , nlevels_y , dof_count = contingency_tble . shape # now compute marginal terms across all degrees of freedom tx_dof = contingency_tble . sum ( axis = 1 ) ty_dof = contingency_tble . sum ( axis = 0 ) # compute sample size within each degree of freedom nk = ty_dof . sum ( axis = 0 ) # compute the term to be logged: # s^{ab}_{ij} * M / (s_i^a s_j^b) tlog = np . zeros (( nlevels_x , nlevels_y , dof_count )) for k in range ( dof_count ): # create a 2x1 and 1x2 array of marginal counts # for each degree of freedom tx = tx_dof [ ... , k ][:, np . newaxis ] ty = ty_dof [ ... , k ][ np . newaxis , :] # compute the final term in the log tdijk = tx . dot ( ty ) tlog [:, :, k ] = contingency_tble [:, :, k ] * nk [ k ] / tdijk log_tlog = np . log ( tlog ) G2 = np . nansum ( 2 * contingency_tble * log_tlog ) return G2 def g_square_binary ( data : Union [ NDArray , pd . DataFrame ], x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , ) -> Tuple [ float , float ]: \"\"\"G square test for a binary data. When running a conditional-independence test, degrees of freecom is calculated. It is defined as ``2^|S|``, where ``|S|`` is the cardinality of the separating set, ``S``. Parameters ---------- data : np.ndarray | pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). Returns ------- G2_stat : float The G^2 statistic. p_val : float The p-value of conditional independence. Notes ----- The ``G^2`` statistic for binary outcome 'a' and 'b' is: .. math:: 2 \\\\times \\sum_{a,b} S^{a,b}_{ij} ln(\\\\frac{s^{ab}_{ij} M}{s_i^a s_j^b}) which takes the sum over occurrences of 'a' and 'b' and multiplies it by the number of samples, M and normalizes it :footcite:`Neapolitan2003`. References ---------- .. footbibliography:: \"\"\" # rng = np.random.RandomState(random_state) if isinstance ( data , np . ndarray ): data = pd . DataFrame ( data ) if any ( xy not in data . columns for xy in [ x , y ]): raise ValueError ( f 'Variables \"x\" ( { x } ) and \"y\" ( { y } ) are not in the columns of \"data\": ' f \" { data . columns . values } .\" ) n_samples = data . shape [ 0 ] s_size = len ( sep_set ) dof = int ( pow ( 2 , s_size )) # check number of samples relative to degrees of freedom # assuming no zeros n_samples_req = 10 * dof if n_samples < n_samples_req : raise RuntimeError ( f \"Not enough samples. { n_samples } is too small. Need { n_samples_req } .\" ) # hard-cut off cardinality of separating set at 6 if s_size < 6 : # set up contingency table for binary data 2x2xdof contingency_tble = _calculate_contingency_tble ( x , y , sep_set , dof , data , 2 , 2 ) else : # s_size >= 6 contingency_tble = _calculate_highdim_contingency ( x , y , sep_set , data , 2 , 2 ) G2_stat = _calculate_g_statistic ( contingency_tble ) p_val = chi2 . sf ( G2_stat , dof ) return G2_stat , p_val def g_square_discrete ( data : Union [ NDArray , pd . DataFrame ], x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , levels = None , ) -> Tuple [ float , float ]: \"\"\"G square test for discrete data. Parameters ---------- data : np.ndarray | pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). levels : list Levels of each column in the data matrix (as a list()). Returns ------- G2 : float The G^2 test statistic. p_val : float the p-value of conditional independence. \"\"\" if isinstance ( data , np . ndarray ): data = pd . DataFrame ( data ) if any ( xy not in data . columns for xy in [ x , y ]): raise ValueError ( f 'Variables \"x\" ( { x } ) and \"y\" ( { y } ) are not in the columns of \"data\": ' f \" { data . columns . values } .\" ) if levels is None : levels = np . amax ( data , axis = 0 ) + 1 n_samples = data . shape [ 0 ] s_size = len ( sep_set ) dof = ( levels [ x ] - 1 ) * ( levels [ y ] - 1 ) * np . prod ( list ( map ( lambda x : levels [ x ], sep_set ))) # check number of samples relative to degrees of freedom n_samples_req = 10 * dof if n_samples < n_samples_req : raise RuntimeError ( f \"Not enough samples. { n_samples } is too small. Need { n_samples_req } .\" ) contingency_tble = None # hard-cut off cardinality of separating set at 5 if s_size < 5 : # degrees of freedom prod_levels = np . prod ( list ( map ( lambda x : levels [ x ], sep_set ))) . astype ( int ) # set up contingency table for binary data # |X| x |Y| x dof contingency_tble = _calculate_contingency_tble ( x , y , sep_set , prod_levels , data , nlevels_x = levels [ x ], nlevels_y = levels [ y ], levels = levels , ) else : # s_size >= 5 contingency_tble = _calculate_highdim_contingency ( x , y , sep_set , data , levels [ x ], levels [ y ]) # calculate the actual G statistic given the contingency table G2 = _calculate_g_statistic ( contingency_tble ) if dof == 0 : # dof can be 0 when levels[x] or levels[y] is 1, which is # the case that the values of columns x or y are all 0. p_val = 1 else : p_val = chi2 . sf ( G2 , dof ) return G2 , p_val class GSquareCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , data_type : str = \"binary\" ): self . data_type = data_type def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None , levels : List = None , ) -> Tuple [ float , float ]: self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () if self . data_type == \"binary\" : stat , pvalue = g_square_binary ( df , x_var , y_var , z_covariates ) elif self . data_type == \"discrete\" : stat , pvalue = g_square_discrete ( df , x_var , y_var , z_covariates , levels = levels ) else : raise ValueError ( f \"The acceptable data_type for G Square CI test is \" f '\"binary\" and \"discrete\", not { self . data_type } .' ) return stat , pvalue Functions g_square_binary def g_square_binary ( data : Union [ numpy . ndarray [ Any , numpy . dtype [ + ScalarType ]], pandas . core . frame . DataFrame ], x : Union [ int , str ], y : Union [ int , str ], sep_set : Set ) -> Tuple [ float , float ] G square test for a binary data. When running a conditional-independence test, degrees of freecom is calculated. It is defined as 2^|S| , where |S| is the cardinality of the separating set, S . Parameters: Name Type Description Default data np.ndarray pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x int str the first node variable. If data is a DataFrame, then 'x' must be in the columns of data . None y int str the second node variable. If data is a DataFrame, then 'y' must be in the columns of data . None sep_set set the set of neibouring nodes of x and y (as a set()). None Returns: Type Description float The G^2 statistic. View Source def g_square_binary ( data : Union [ NDArray , pd . DataFrame ] , x : Union [ int , str ] , y : Union [ int , str ] , sep_set : Set , ) -> Tuple [ float , float ] : \" \"\" G square test for a binary data. When running a conditional-independence test, degrees of freecom is calculated. It is defined as ``2^|S|``, where ``|S|`` is the cardinality of the separating set, ``S``. Parameters ---------- data : np.ndarray | pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). Returns ------- G2_stat : float The G^2 statistic. p_val : float The p-value of conditional independence. Notes ----- The ``G^2`` statistic for binary outcome 'a' and 'b' is: .. math:: 2 \\\\ times \\ sum_{a,b} S^{a,b}_{ij} ln( \\\\ frac{s^{ab}_{ij} M}{s_i^a s_j^b}) which takes the sum over occurrences of 'a' and 'b' and multiplies it by the number of samples, M and normalizes it :footcite:`Neapolitan2003`. References ---------- .. footbibliography:: \"\" \" # rng = np.random.RandomState(random_state) if isinstance ( data , np . ndarray ) : data = pd . DataFrame ( data ) if any ( xy not in data . columns for xy in [ x , y ] ) : raise ValueError ( f 'Variables \"x\" ({x}) and \"y\" ({y}) are not in the columns of \"data\": ' f \"{data.columns.values}.\" ) n_samples = data . shape [ 0 ] s_size = len ( sep_set ) dof = int ( pow ( 2 , s_size )) # check number of samples relative to degrees of freedom # assuming no zeros n_samples_req = 10 * dof if n_samples < n_samples_req : raise RuntimeError ( f \"Not enough samples. {n_samples} is too small. Need {n_samples_req}.\" ) # hard-cut off cardinality of separating set at 6 if s_size < 6 : # set up contingency table for binary data 2x2xdof contingency_tble = _calculate_contingency_tble ( x , y , sep_set , dof , data , 2 , 2 ) else : # s_size >= 6 contingency_tble = _calculate_highdim_contingency ( x , y , sep_set , data , 2 , 2 ) G2_stat = _calculate_g_statistic ( contingency_tble ) p_val = chi2 . sf ( G2_stat , dof ) return G2_stat , p_val g_square_discrete def g_square_discrete ( data : Union [ numpy . ndarray [ Any , numpy . dtype [ + ScalarType ]], pandas . core . frame . DataFrame ], x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , levels = None ) -> Tuple [ float , float ] G square test for discrete data. Parameters: Name Type Description Default data np.ndarray pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x int str the first node variable. If data is a DataFrame, then 'x' must be in the columns of data . None y int str the second node variable. If data is a DataFrame, then 'y' must be in the columns of data . None sep_set set the set of neibouring nodes of x and y (as a set()). None levels list Levels of each column in the data matrix (as a list()). None Returns: Type Description float The G^2 test statistic. View Source def g_square_discrete ( data : Union [ NDArray, pd.DataFrame ] , x : Union [ int, str ] , y : Union [ int, str ] , sep_set : Set , levels = None , ) -> Tuple [ float, float ] : \"\"\"G square test for discrete data. Parameters ---------- data : np.ndarray | pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). levels : list Levels of each column in the data matrix (as a list()). Returns ------- G2 : float The G^2 test statistic. p_val : float the p-value of conditional independence. \"\"\" if isinstance ( data , np . ndarray ) : data = pd . DataFrame ( data ) if any ( xy not in data . columns for xy in [ x, y ] ) : raise ValueError ( f 'Variables \"x\" ({x}) and \"y\" ({y}) are not in the columns of \"data\": ' f \"{data.columns.values}.\" ) if levels is None : levels = np . amax ( data , axis = 0 ) + 1 n_samples = data . shape [ 0 ] s_size = len ( sep_set ) dof = ( levels [ x ] - 1 ) * ( levels [ y ] - 1 ) * np . prod ( list ( map ( lambda x : levels [ x ] , sep_set ))) # check number of samples relative to degrees of freedom n_samples_req = 10 * dof if n_samples < n_samples_req : raise RuntimeError ( f \"Not enough samples. {n_samples} is too small. Need {n_samples_req}.\" ) contingency_tble = None # hard - cut off cardinality of separating set at 5 if s_size < 5 : # degrees of freedom prod_levels = np . prod ( list ( map ( lambda x : levels [ x ] , sep_set ))). astype ( int ) # set up contingency table for binary data # | X | x | Y | x dof contingency_tble = _calculate_contingency_tble ( x , y , sep_set , prod_levels , data , nlevels_x = levels [ x ] , nlevels_y = levels [ y ] , levels = levels , ) else : # s_size >= 5 contingency_tble = _calculate_highdim_contingency ( x , y , sep_set , data , levels [ x ] , levels [ y ] ) # calculate the actual G statistic given the contingency table G2 = _calculate_g_statistic ( contingency_tble ) if dof == 0 : # dof can be 0 when levels [ x ] or levels [ y ] is 1 , which is # the case that the values of columns x or y are all 0. p_val = 1 else : p_val = chi2 . sf ( G2 , dof ) return G2 , p_val Classes GSquareCITest class GSquareCITest ( data_type : str = 'binary' ) View Source class GSquareCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , data_type : str = \"binary\" ): self . data_type = data_type def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None , levels : List = None , ) -> Tuple [ float , float ]: self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () if self . data_type == \"binary\" : stat , pvalue = g_square_binary ( df , x_var , y_var , z_covariates ) elif self . data_type == \"discrete\" : stat , pvalue = g_square_discrete ( df , x_var , y_var , z_covariates , levels = levels ) else : raise ValueError ( f \"The acceptable data_type for G Square CI test is \" f '\"binary\" and \"discrete\", not {self.data_type}.' ) return stat , pvalue Ancestors (in MRO) dodiscover.ci.base.BaseConditionalIndependenceTest Methods test def test ( self , df : pandas . core . frame . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None , levels : List = None ) -> Tuple [ float , float ] Abstract method for all conditional independence tests. Parameters: Name Type Description Default df pd.DataFrame description None x_var Any description None y_var Any description None z_covariates Any description , by default None None Returns: Type Description Tuple[float, float] description View Source def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None , levels : List = None , ) -> Tuple [ float , float ]: self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () if self . data_type == \"binary\" : stat , pvalue = g_square_binary ( df , x_var , y_var , z_covariates ) elif self . data_type == \"discrete\" : stat , pvalue = g_square_discrete ( df , x_var , y_var , z_covariates , levels = levels ) else : raise ValueError ( f \"The acceptable data_type for G Square CI test is \" f '\"binary\" and \"discrete\", not {self.data_type}.' ) return stat , pvalue","title":"G Test"},{"location":"reference/dodiscover/ci/g_test/#module-dodiscovercig_test","text":"None None View Source # This code was originally adapted from https://github.com/keiichishima/gsq # and heavily refactored and modified. from typing import Any , List , Set , Tuple , Union import numpy as np import pandas as pd from numpy.typing import NDArray from scipy.stats import chi2 from .base import BaseConditionalIndependenceTest def _calculate_contingency_tble ( x : Union [ int , str ], y : Union [ int , str ], sep_set : Union [ List , Set ], dof : int , data : NDArray , nlevels_x : int , nlevels_y : int , levels : NDArray = None , ) -> NDArray : \"\"\"Calculate log term for binary G^2 statistic in CI test. Computes the contingency table and the associated log-term within the G^2 statistic for binary data. Parameters ---------- x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). dof : int The degrees of freedom. data : np.ndarray of shape (n_samples, n_variables) The input data matrix. nlevels_x : int The number of levels in the 'x' data variable. nlevels_y : int The number of levels in the 'y' data variable. levels : np.ndarray of shape (n_variables,) The number of levels associated with each variable. Returns ------- contingency_tble : np.ndarray of shape (2, 2, dof) A contingency table per degree of freedom. \"\"\" # define contingency table as a 2 by 2 table relating 'x' and 'y' # across different separating set variables contingency_tble = np . zeros (( nlevels_x , nlevels_y , dof )) x_idx = data [ x ] # [:, x] y_idx = data [ y ] # [:, y] sep_set = list ( sep_set ) # sum all co-occurrences of x and y conditioned on z for row_idx , ( idx , jdx ) in enumerate ( zip ( x_idx , y_idx )): kdx = 0 for zidx , z in enumerate ( sep_set ): if levels is None : # binary case kdx += data [ z ][ row_idx ] * int ( pow ( 2 , zidx )) else : # discrete case if zidx == 0 : kdx += data [ z ][ row_idx ] # data[row_idx, z] else : lprod = np . prod ( list ( map ( lambda x : levels [ x ], sep_set [: zidx ]))) # type: ignore kdx += data [ z ][ row_idx ] * lprod # increment the co-occurrence found contingency_tble [ idx , jdx , kdx ] += 1 return contingency_tble def _calculate_highdim_contingency ( x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , data : NDArray , nlevel_x : int , nlevels_y : int , ) -> NDArray : \"\"\"Calculate the contingency table for \"large\" separating set. When separating set is deemed \"large\", we use a different approach to computing the overall contingency table. Parameters ---------- x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). data : np.ndarray of shape (n_samples, n_variables) The input data matrix. nlevel_x : int Number of levels of the 'x' variable in the data matrix. nlevels_y : int Number of levels of the 'y' variable in the data matrix. Returns ------- contingency_tble : np.ndarray of shape (nlevel_x, nlevel_y, dof) A contingency table per degree of freedom per level of each variable. \"\"\" n_samples , _ = data . shape # keep track of all variables in the separating set sep_set = list ( sep_set ) # type: ignore k = data [:, sep_set ] # count number of value combinations for sepset variables # observed in the data dof_count = 1 parents_val = np . array ([ k [ 0 , :]]) # initialize the contingency table contingency_tble = np . zeros (( 2 , 2 , 1 )) xdx = data [ 0 , x ] ydx = data [ 0 , y ] contingency_tble [ xdx , ydx , dof_count - 1 ] = 1 # check how many parents we can create from the rest of the dataset for idx in range ( 1 , n_samples ): is_new = True xdx = data [ idx , x ] ydx = data [ idx , y ] # comparing the current values of the subset variables to all # already existing combinations of subset variables values tcomp = parents_val [: dof_count , :] == k [ idx , :] for it_parents in range ( dof_count ): if np . all ( tcomp [ it_parents , :]): contingency_tble [ xdx , ydx , it_parents ] += 1 is_new = False break # new combination of separating set values, so we create a new # contingency table if is_new : dof_count += 1 parents_val = np . r_ [ parents_val , [ k [ idx , :]]] # create a new contingnecy table and update cell counts # using the original table up to the last value ncontingency_tble = np . zeros (( nlevel_x , nlevels_y , dof_count )) for p in range ( dof_count - 1 ): ncontingency_tble [:, :, p ] = contingency_tble [:, :, p ] ncontingency_tble [ xdx , ydx , dof_count - 1 ] = 1 contingency_tble = ncontingency_tble return contingency_tble def _calculate_g_statistic ( contingency_tble ): \"\"\"Calculate a G statistic from contingency table. Parameters ---------- contingency_tble : np.ndarray of shape (nlevels_x, nlevels_y, dof) The contingency table of 'x' vs 'y'. Returns ------- G2 : float G^2 test statistic. \"\"\" nlevels_x , nlevels_y , dof_count = contingency_tble . shape # now compute marginal terms across all degrees of freedom tx_dof = contingency_tble . sum ( axis = 1 ) ty_dof = contingency_tble . sum ( axis = 0 ) # compute sample size within each degree of freedom nk = ty_dof . sum ( axis = 0 ) # compute the term to be logged: # s^{ab}_{ij} * M / (s_i^a s_j^b) tlog = np . zeros (( nlevels_x , nlevels_y , dof_count )) for k in range ( dof_count ): # create a 2x1 and 1x2 array of marginal counts # for each degree of freedom tx = tx_dof [ ... , k ][:, np . newaxis ] ty = ty_dof [ ... , k ][ np . newaxis , :] # compute the final term in the log tdijk = tx . dot ( ty ) tlog [:, :, k ] = contingency_tble [:, :, k ] * nk [ k ] / tdijk log_tlog = np . log ( tlog ) G2 = np . nansum ( 2 * contingency_tble * log_tlog ) return G2 def g_square_binary ( data : Union [ NDArray , pd . DataFrame ], x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , ) -> Tuple [ float , float ]: \"\"\"G square test for a binary data. When running a conditional-independence test, degrees of freecom is calculated. It is defined as ``2^|S|``, where ``|S|`` is the cardinality of the separating set, ``S``. Parameters ---------- data : np.ndarray | pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). Returns ------- G2_stat : float The G^2 statistic. p_val : float The p-value of conditional independence. Notes ----- The ``G^2`` statistic for binary outcome 'a' and 'b' is: .. math:: 2 \\\\times \\sum_{a,b} S^{a,b}_{ij} ln(\\\\frac{s^{ab}_{ij} M}{s_i^a s_j^b}) which takes the sum over occurrences of 'a' and 'b' and multiplies it by the number of samples, M and normalizes it :footcite:`Neapolitan2003`. References ---------- .. footbibliography:: \"\"\" # rng = np.random.RandomState(random_state) if isinstance ( data , np . ndarray ): data = pd . DataFrame ( data ) if any ( xy not in data . columns for xy in [ x , y ]): raise ValueError ( f 'Variables \"x\" ( { x } ) and \"y\" ( { y } ) are not in the columns of \"data\": ' f \" { data . columns . values } .\" ) n_samples = data . shape [ 0 ] s_size = len ( sep_set ) dof = int ( pow ( 2 , s_size )) # check number of samples relative to degrees of freedom # assuming no zeros n_samples_req = 10 * dof if n_samples < n_samples_req : raise RuntimeError ( f \"Not enough samples. { n_samples } is too small. Need { n_samples_req } .\" ) # hard-cut off cardinality of separating set at 6 if s_size < 6 : # set up contingency table for binary data 2x2xdof contingency_tble = _calculate_contingency_tble ( x , y , sep_set , dof , data , 2 , 2 ) else : # s_size >= 6 contingency_tble = _calculate_highdim_contingency ( x , y , sep_set , data , 2 , 2 ) G2_stat = _calculate_g_statistic ( contingency_tble ) p_val = chi2 . sf ( G2_stat , dof ) return G2_stat , p_val def g_square_discrete ( data : Union [ NDArray , pd . DataFrame ], x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , levels = None , ) -> Tuple [ float , float ]: \"\"\"G square test for discrete data. Parameters ---------- data : np.ndarray | pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). levels : list Levels of each column in the data matrix (as a list()). Returns ------- G2 : float The G^2 test statistic. p_val : float the p-value of conditional independence. \"\"\" if isinstance ( data , np . ndarray ): data = pd . DataFrame ( data ) if any ( xy not in data . columns for xy in [ x , y ]): raise ValueError ( f 'Variables \"x\" ( { x } ) and \"y\" ( { y } ) are not in the columns of \"data\": ' f \" { data . columns . values } .\" ) if levels is None : levels = np . amax ( data , axis = 0 ) + 1 n_samples = data . shape [ 0 ] s_size = len ( sep_set ) dof = ( levels [ x ] - 1 ) * ( levels [ y ] - 1 ) * np . prod ( list ( map ( lambda x : levels [ x ], sep_set ))) # check number of samples relative to degrees of freedom n_samples_req = 10 * dof if n_samples < n_samples_req : raise RuntimeError ( f \"Not enough samples. { n_samples } is too small. Need { n_samples_req } .\" ) contingency_tble = None # hard-cut off cardinality of separating set at 5 if s_size < 5 : # degrees of freedom prod_levels = np . prod ( list ( map ( lambda x : levels [ x ], sep_set ))) . astype ( int ) # set up contingency table for binary data # |X| x |Y| x dof contingency_tble = _calculate_contingency_tble ( x , y , sep_set , prod_levels , data , nlevels_x = levels [ x ], nlevels_y = levels [ y ], levels = levels , ) else : # s_size >= 5 contingency_tble = _calculate_highdim_contingency ( x , y , sep_set , data , levels [ x ], levels [ y ]) # calculate the actual G statistic given the contingency table G2 = _calculate_g_statistic ( contingency_tble ) if dof == 0 : # dof can be 0 when levels[x] or levels[y] is 1, which is # the case that the values of columns x or y are all 0. p_val = 1 else : p_val = chi2 . sf ( G2 , dof ) return G2 , p_val class GSquareCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , data_type : str = \"binary\" ): self . data_type = data_type def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None , levels : List = None , ) -> Tuple [ float , float ]: self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () if self . data_type == \"binary\" : stat , pvalue = g_square_binary ( df , x_var , y_var , z_covariates ) elif self . data_type == \"discrete\" : stat , pvalue = g_square_discrete ( df , x_var , y_var , z_covariates , levels = levels ) else : raise ValueError ( f \"The acceptable data_type for G Square CI test is \" f '\"binary\" and \"discrete\", not { self . data_type } .' ) return stat , pvalue","title":"Module dodiscover.ci.g_test"},{"location":"reference/dodiscover/ci/g_test/#functions","text":"","title":"Functions"},{"location":"reference/dodiscover/ci/g_test/#g_square_binary","text":"def g_square_binary ( data : Union [ numpy . ndarray [ Any , numpy . dtype [ + ScalarType ]], pandas . core . frame . DataFrame ], x : Union [ int , str ], y : Union [ int , str ], sep_set : Set ) -> Tuple [ float , float ] G square test for a binary data. When running a conditional-independence test, degrees of freecom is calculated. It is defined as 2^|S| , where |S| is the cardinality of the separating set, S . Parameters: Name Type Description Default data np.ndarray pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x int str the first node variable. If data is a DataFrame, then 'x' must be in the columns of data . None y int str the second node variable. If data is a DataFrame, then 'y' must be in the columns of data . None sep_set set the set of neibouring nodes of x and y (as a set()). None Returns: Type Description float The G^2 statistic. View Source def g_square_binary ( data : Union [ NDArray , pd . DataFrame ] , x : Union [ int , str ] , y : Union [ int , str ] , sep_set : Set , ) -> Tuple [ float , float ] : \" \"\" G square test for a binary data. When running a conditional-independence test, degrees of freecom is calculated. It is defined as ``2^|S|``, where ``|S|`` is the cardinality of the separating set, ``S``. Parameters ---------- data : np.ndarray | pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). Returns ------- G2_stat : float The G^2 statistic. p_val : float The p-value of conditional independence. Notes ----- The ``G^2`` statistic for binary outcome 'a' and 'b' is: .. math:: 2 \\\\ times \\ sum_{a,b} S^{a,b}_{ij} ln( \\\\ frac{s^{ab}_{ij} M}{s_i^a s_j^b}) which takes the sum over occurrences of 'a' and 'b' and multiplies it by the number of samples, M and normalizes it :footcite:`Neapolitan2003`. References ---------- .. footbibliography:: \"\" \" # rng = np.random.RandomState(random_state) if isinstance ( data , np . ndarray ) : data = pd . DataFrame ( data ) if any ( xy not in data . columns for xy in [ x , y ] ) : raise ValueError ( f 'Variables \"x\" ({x}) and \"y\" ({y}) are not in the columns of \"data\": ' f \"{data.columns.values}.\" ) n_samples = data . shape [ 0 ] s_size = len ( sep_set ) dof = int ( pow ( 2 , s_size )) # check number of samples relative to degrees of freedom # assuming no zeros n_samples_req = 10 * dof if n_samples < n_samples_req : raise RuntimeError ( f \"Not enough samples. {n_samples} is too small. Need {n_samples_req}.\" ) # hard-cut off cardinality of separating set at 6 if s_size < 6 : # set up contingency table for binary data 2x2xdof contingency_tble = _calculate_contingency_tble ( x , y , sep_set , dof , data , 2 , 2 ) else : # s_size >= 6 contingency_tble = _calculate_highdim_contingency ( x , y , sep_set , data , 2 , 2 ) G2_stat = _calculate_g_statistic ( contingency_tble ) p_val = chi2 . sf ( G2_stat , dof ) return G2_stat , p_val","title":"g_square_binary"},{"location":"reference/dodiscover/ci/g_test/#g_square_discrete","text":"def g_square_discrete ( data : Union [ numpy . ndarray [ Any , numpy . dtype [ + ScalarType ]], pandas . core . frame . DataFrame ], x : Union [ int , str ], y : Union [ int , str ], sep_set : Set , levels = None ) -> Tuple [ float , float ] G square test for discrete data. Parameters: Name Type Description Default data np.ndarray pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x int str the first node variable. If data is a DataFrame, then 'x' must be in the columns of data . None y int str the second node variable. If data is a DataFrame, then 'y' must be in the columns of data . None sep_set set the set of neibouring nodes of x and y (as a set()). None levels list Levels of each column in the data matrix (as a list()). None Returns: Type Description float The G^2 test statistic. View Source def g_square_discrete ( data : Union [ NDArray, pd.DataFrame ] , x : Union [ int, str ] , y : Union [ int, str ] , sep_set : Set , levels = None , ) -> Tuple [ float, float ] : \"\"\"G square test for discrete data. Parameters ---------- data : np.ndarray | pandas.DataFrame of shape (n_samples, n_variables) The data matrix to be used. x : int | str the first node variable. If ``data`` is a DataFrame, then 'x' must be in the columns of ``data``. y : int | str the second node variable. If ``data`` is a DataFrame, then 'y' must be in the columns of ``data``. sep_set : set the set of neibouring nodes of x and y (as a set()). levels : list Levels of each column in the data matrix (as a list()). Returns ------- G2 : float The G^2 test statistic. p_val : float the p-value of conditional independence. \"\"\" if isinstance ( data , np . ndarray ) : data = pd . DataFrame ( data ) if any ( xy not in data . columns for xy in [ x, y ] ) : raise ValueError ( f 'Variables \"x\" ({x}) and \"y\" ({y}) are not in the columns of \"data\": ' f \"{data.columns.values}.\" ) if levels is None : levels = np . amax ( data , axis = 0 ) + 1 n_samples = data . shape [ 0 ] s_size = len ( sep_set ) dof = ( levels [ x ] - 1 ) * ( levels [ y ] - 1 ) * np . prod ( list ( map ( lambda x : levels [ x ] , sep_set ))) # check number of samples relative to degrees of freedom n_samples_req = 10 * dof if n_samples < n_samples_req : raise RuntimeError ( f \"Not enough samples. {n_samples} is too small. Need {n_samples_req}.\" ) contingency_tble = None # hard - cut off cardinality of separating set at 5 if s_size < 5 : # degrees of freedom prod_levels = np . prod ( list ( map ( lambda x : levels [ x ] , sep_set ))). astype ( int ) # set up contingency table for binary data # | X | x | Y | x dof contingency_tble = _calculate_contingency_tble ( x , y , sep_set , prod_levels , data , nlevels_x = levels [ x ] , nlevels_y = levels [ y ] , levels = levels , ) else : # s_size >= 5 contingency_tble = _calculate_highdim_contingency ( x , y , sep_set , data , levels [ x ] , levels [ y ] ) # calculate the actual G statistic given the contingency table G2 = _calculate_g_statistic ( contingency_tble ) if dof == 0 : # dof can be 0 when levels [ x ] or levels [ y ] is 1 , which is # the case that the values of columns x or y are all 0. p_val = 1 else : p_val = chi2 . sf ( G2 , dof ) return G2 , p_val","title":"g_square_discrete"},{"location":"reference/dodiscover/ci/g_test/#classes","text":"","title":"Classes"},{"location":"reference/dodiscover/ci/g_test/#gsquarecitest","text":"class GSquareCITest ( data_type : str = 'binary' ) View Source class GSquareCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , data_type : str = \"binary\" ): self . data_type = data_type def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None , levels : List = None , ) -> Tuple [ float , float ]: self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () if self . data_type == \"binary\" : stat , pvalue = g_square_binary ( df , x_var , y_var , z_covariates ) elif self . data_type == \"discrete\" : stat , pvalue = g_square_discrete ( df , x_var , y_var , z_covariates , levels = levels ) else : raise ValueError ( f \"The acceptable data_type for G Square CI test is \" f '\"binary\" and \"discrete\", not {self.data_type}.' ) return stat , pvalue","title":"GSquareCITest"},{"location":"reference/dodiscover/ci/g_test/#ancestors-in-mro","text":"dodiscover.ci.base.BaseConditionalIndependenceTest","title":"Ancestors (in MRO)"},{"location":"reference/dodiscover/ci/g_test/#methods","text":"","title":"Methods"},{"location":"reference/dodiscover/ci/g_test/#test","text":"def test ( self , df : pandas . core . frame . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None , levels : List = None ) -> Tuple [ float , float ] Abstract method for all conditional independence tests. Parameters: Name Type Description Default df pd.DataFrame description None x_var Any description None y_var Any description None z_covariates Any description , by default None None Returns: Type Description Tuple[float, float] description View Source def test ( self , df : pd . DataFrame , x_var : Any , y_var : Any , z_covariates : Any = None , levels : List = None , ) -> Tuple [ float , float ]: self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None : z_covariates = set () if self . data_type == \"binary\" : stat , pvalue = g_square_binary ( df , x_var , y_var , z_covariates ) elif self . data_type == \"discrete\" : stat , pvalue = g_square_discrete ( df , x_var , y_var , z_covariates , levels = levels ) else : raise ValueError ( f \"The acceptable data_type for G Square CI test is \" f '\"binary\" and \"discrete\", not {self.data_type}.' ) return stat , pvalue","title":"test"},{"location":"reference/dodiscover/ci/kernel_test/","text":"Module dodiscover.ci.kernel_test None None View Source import numpy as np from scipy import stats from sklearn.metrics import pairwise_distances , pairwise_kernels from sklearn.metrics.pairwise import PAIRWISE_KERNEL_FUNCTIONS from .base import BaseConditionalIndependenceTest class KernelCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , kernel_x : str = \"rbf\" , kernel_y : str = \"rbf\" , kernel_z : str = \"rbf\" , null_size : int = 1000 , approx_with_gamma : bool = True , kwidth_x = None , kwidth_y = None , kwidth_z = None , threshold : float = 1e-5 , n_jobs : int = None , ): \"\"\"Kernel (Conditional) Independence Test. For testing (conditional) independence on continuous data, we leverage kernels :footcite:`Zhang2011` that are computationally efficient. Parameters ---------- kernel_x : str, optional The kernel function for data 'X', by default \"rbf\". kernel_y : str, optional The kernel function for data 'Y', by default \"rbf\". kernel_z : str, optional The kernel function for data 'Z', by default \"rbf\". null_size : int, optional The number of samples to generate for the bootstrap distribution to approximate the pvalue, by default 1000. approx_with_gamma : bool, optional Whether to use the Gamma distribution approximation for the pvalue, by default True. kwidth_x : _type_, optional _description_, by default None kwidth_y : _type_, optional _description_, by default None kwidth_z : _type_, optional _description_, by default None threshold : float, optional The threshold set on the value of eigenvalues, by default 1e-5. Used to regularize the method. n_jobs : int, optional The number of CPUs to use, by default None. Notes ----- Valid strings for ``compute_kernel`` are, as defined in :func:`sklearn.metrics.pairwise.pairwise_kernels`, [``\"additive_chi2\"``, ``\"chi2\"``, ``\"linear\"``, ``\"poly\"``, ``\"polynomial\"``, ``\"rbf\"``, ``\"laplacian\"``, ``\"sigmoid\"``, ``\"cosine\"``] References ---------- .. footbibliography:: \"\"\" if isinstance ( kernel_x , str ) and kernel_x not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are { PAIRWISE_KERNEL_FUNCTIONS } . \" f \"You passed in { kernel_x } for kernel_x.\" ) if isinstance ( kernel_y , str ) and kernel_y not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are { PAIRWISE_KERNEL_FUNCTIONS } . \" f \"You passed in { kernel_y } for kernel_y.\" ) if isinstance ( kernel_z , str ) and kernel_z not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are { PAIRWISE_KERNEL_FUNCTIONS } . \" f \"You passed in { kernel_z } for kernel_z.\" ) self . kernel_x = kernel_x self . kernel_y = kernel_y self . kernel_z = kernel_z self . null_size = null_size self . approx_with_gamma = approx_with_gamma self . threshold = threshold self . n_jobs = n_jobs # hyperparameters of the kernsl self . kwidth_x = kwidth_x self . kwidth_y = kwidth_y self . kwidth_z = kwidth_z def test ( self , df , x_var , y_var , z_covariates = None ): self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None or len ( z_covariates ) == 0 : Z = None else : z_covariates = list ( z_covariates ) Z = df [ z_covariates ] . to_numpy () . reshape (( - 1 , len ( z_covariates ))) X = df [ x_var ] . to_numpy ()[:, np . newaxis ] Y = df [ y_var ] . to_numpy ()[:, np . newaxis ] # first normalize the data to have zero mean and unit variance # along the columns of the data X = stats . zscore ( X , axis = 0 ) Y = stats . zscore ( Y , axis = 0 ) if Z is not None : Z = stats . zscore ( Z , axis = 0 ) # when running CI, \\ddot{X} comprises of (X, Z) X = np . concatenate (( X , Z ), axis = 1 ) Kz , sigma_z = self . _compute_kernel ( Z , distance_metric = \"l2\" , metric = self . kernel_z , kwidth = self . kwidth_z , centered = True , ) # compute the centralized kernel matrices of each the datasets Kx , sigma_x = self . _compute_kernel ( X , distance_metric = \"l2\" , metric = self . kernel_x , kwidth = self . kwidth_x , centered = True , ) Ky , sigma_y = self . _compute_kernel ( Y , distance_metric = \"l2\" , metric = self . kernel_y , kwidth = self . kwidth_y , centered = True ) if Z is None : # test statistic is just the normal bivariate independence # test statistic test_stat = self . _compute_V_statistic ( Kx , Ky ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ind ( Kx , Ky ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ind ( Kx , Ky , n_samples = self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) else : # compute the centralizing matrix for the kernels according to # conditioning set Z epsilon = 1e-7 n = Kx . shape [ 0 ] Rz = epsilon * np . linalg . pinv ( Kz + epsilon * np . eye ( n )) # compute the centralized kernel matrices KxzR = Rz . dot ( Kx ) . dot ( Rz ) KyzR = Rz . dot ( Ky ) . dot ( Rz ) # compute the conditional independence test statistic test_stat = self . _compute_V_statistic ( KxzR , KyzR ) # compute the product of the eigenvectors uu_prod = self . _compute_prod_eigvecs ( KxzR , KyzR , threshold = self . threshold ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ci ( uu_prod ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ci ( uu_prod , self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) return test_stat , pvalue def _approx_gamma_params_ind ( self , Kx , Ky ): T = Kx . shape [ 0 ] mean_appr = np . trace ( Kx ) * np . trace ( Ky ) / T var_appr = 2 * np . trace ( Kx . dot ( Kx )) * np . trace ( Ky . dot ( Ky )) / T / T k_appr = mean_appr ** 2 / var_appr theta_appr = var_appr / mean_appr return k_appr , theta_appr def _approx_gamma_params_ci ( self , uu_prod ): \"\"\"Get parameters of the approximated Gamma distribution. Parameters ---------- uu_prod : np.ndarray of shape (n_features, n_features) The product of the eigenvectors of Kx and Ky, the kernels on the input data, X and Y. Returns ------- k_appr : float The shape parameter of the Gamma distribution. theta_appr : float The scale parameter of the Gamma distribution. Notes ----- X ~ Gamma(k, theta) with a probability density function of the following: .. math:: f(x; k, \\\\theta) = \\\\frac{x^{k-1} e^{-x / \\\\theta}}{\\\\theta^k \\\\Gamma(k)} where $\\\\Gamma(k)$ is the Gamma function evaluated at k. In this scenario k governs the shape of the pdf, while $\\\\theta$ governs more how spread out the data is. \"\"\" # approximate the mean and the variance mean_appr = np . trace ( uu_prod ) var_appr = 2 * np . trace ( uu_prod . dot ( uu_prod )) k_appr = mean_appr ** 2 / var_appr theta_appr = var_appr / mean_appr return k_appr , theta_appr def _compute_prod_eigvecs ( self , Kx , Ky , threshold = None ): T = Kx . shape [ 0 ] wx , vx = np . linalg . eigh ( 0.5 * ( Kx + Kx . T )) wy , vy = np . linalg . eigh ( 0.5 * ( Ky + Ky . T )) if threshold is not None : # threshold eigenvalues that are below a certain threshold # and remove their corresponding values and eigenvectors vx = vx [:, wx > np . max ( wx ) * threshold ] wx = wx [ wx > np . max ( wx ) * threshold ] vy = vy [:, wy > np . max ( wy ) * threshold ] wy = wy [ wy > np . max ( wy ) * threshold ] # scale the eigenvectors by their eigenvalues vx = vx . dot ( np . diag ( np . sqrt ( wx ))) vy = vy . dot ( np . diag ( np . sqrt ( wy ))) # compute the product of the scaled eigenvectors num_eigx = vx . shape [ 1 ] num_eigy = vy . shape [ 1 ] size_u = num_eigx * num_eigy uu = np . zeros (( T , size_u )) for i in range ( 0 , num_eigx ): for j in range ( 0 , num_eigy ): # compute the dot product of eigenvectors uu [:, i * num_eigy + j ] = vx [:, i ] * vy [:, j ] # now compute the product if size_u > T : uu_prod = uu . dot ( uu . T ) else : uu_prod = uu . T . dot ( uu ) return uu_prod def _compute_V_statistic ( self , KxR , KyR ): # n = KxR.shape[0] # compute the sum of the two kernsl Vstat = np . sum ( KxR * KyR ) return Vstat def _compute_null_ind ( self , Kx , Ky , n_samples , max_num_eigs = 1000 ): n = Kx . shape [ 0 ] # get the eigenvalues in ascending order, smallest to largest eigvals_x = np . linalg . eigvalsh ( Kx ) eigvals_y = np . linalg . eigvalsh ( Ky ) # optionally only keep the largest \"N\" eigenvalues eigvals_x = eigvals_x [ - max_num_eigs :] eigvals_y = eigvals_y [ - max_num_eigs :] num_eigs = len ( eigvals_x ) # compute the entry-wise product of the eigenvalues and store it as a vector eigvals_prod = np . dot ( eigvals_x . reshape ( num_eigs , 1 ), eigvals_y . reshape ( 1 , num_eigs ) ) . reshape (( - 1 , 1 )) # only keep eigenvalues above a certain threshold eigvals_prod = eigvals_prod [ eigvals_prod > eigvals_prod . max () * self . threshold ] # generate chi-square distributed values z_{ij} with degree of freedom 1 f_rand = np . random . chisquare ( df = 1 , size = ( len ( eigvals_prod ), n_samples )) # compute the null distribution consisting now of (n_samples) # of chi-squared random variables weighted by the eigenvalue products null_dist = 1.0 / n * eigvals_prod . T . dot ( f_rand ) return null_dist def _compute_null_ci ( self , uu_prod , n_samples ): # the eigenvalues of ww^T eig_uu = np . linalg . eigvalsh ( uu_prod ) eig_uu = eig_uu [ eig_uu > eig_uu . max () * self . threshold ] # generate chi-square distributed values z_{ij} with degree of freedom 1 f_rand = np . random . chisquare ( df = 1 , size = ( eig_uu . shape [ 0 ], n_samples )) # compute the null distribution consisting now of (n_samples) # of chi-squared random variables weighted by the eigenvalue products null_dist = eig_uu . T . dot ( f_rand ) return null_dist def _compute_kernel ( self , X , Y = None , distance_metric = \"l2\" , metric = \"rbf\" , kwidth = None , centered = True ): # if the width of the kernel is not set, then use the median trick to set the # kernel width based on the data X if kwidth is None : # Note: sigma = 1 / np.sqrt(kwidth) # compute N x N pairwise distance matrix dists = pairwise_distances ( X , metric = distance_metric , n_jobs = self . n_jobs ) # compute median of off diagonal elements med = np . median ( dists [ dists > 0 ]) # prevents division by zero when used on label vectors med = med if med else 1 else : med = kwidth extra_kwargs = dict () if metric == \"rbf\" : # compute the normalization factor of the width of the Gaussian kernel gamma = 1.0 / ( 2 * ( med ** 2 )) extra_kwargs [ \"gamma\" ] = gamma elif metric == \"polynomial\" : degree = 2 extra_kwargs [ \"degree\" ] = degree # compute the potentially pairwise kernel kernel = pairwise_kernels ( X , Y = Y , metric = metric , n_jobs = self . n_jobs , ** extra_kwargs ) if centered : kernel = self . _center_kernel ( kernel ) return kernel , med def _center_kernel ( self , K ): \"\"\"Centers the kernel matrix. Applies a transformation H * K * H, where H is a diagonal matrix with 1/n along the diagonal. \"\"\" n = K . shape [ 0 ] H = np . eye ( n ) - 1.0 / n return H . dot ( K ) . dot ( H ) Variables PAIRWISE_KERNEL_FUNCTIONS Classes KernelCITest class KernelCITest ( kernel_x : str = 'rbf' , kernel_y : str = 'rbf' , kernel_z : str = 'rbf' , null_size : int = 1000 , approx_with_gamma : bool = True , kwidth_x = None , kwidth_y = None , kwidth_z = None , threshold : float = 1e-05 , n_jobs : int = None ) View Source class KernelCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , kernel_x : str = \"rbf\" , kernel_y : str = \"rbf\" , kernel_z : str = \"rbf\" , null_size : int = 1000 , approx_with_gamma : bool = True , kwidth_x = None , kwidth_y = None , kwidth_z = None , threshold : float = 1e-5 , n_jobs : int = None , ): \"\"\"Kernel (Conditional) Independence Test. For testing (conditional) independence on continuous data, we leverage kernels :footcite:`Zhang2011` that are computationally efficient. Parameters ---------- kernel_x : str, optional The kernel function for data 'X', by default \"rbf\". kernel_y : str, optional The kernel function for data 'Y', by default \"rbf\". kernel_z : str, optional The kernel function for data 'Z', by default \"rbf\". null_size : int, optional The number of samples to generate for the bootstrap distribution to approximate the pvalue, by default 1000. approx_with_gamma : bool, optional Whether to use the Gamma distribution approximation for the pvalue, by default True. kwidth_x : _type_, optional _description_, by default None kwidth_y : _type_, optional _description_, by default None kwidth_z : _type_, optional _description_, by default None threshold : float, optional The threshold set on the value of eigenvalues, by default 1e-5. Used to regularize the method. n_jobs : int, optional The number of CPUs to use, by default None. Notes ----- Valid strings for ``compute_kernel`` are, as defined in :func:`sklearn.metrics.pairwise.pairwise_kernels`, [``\"additive_chi2\"``, ``\"chi2\"``, ``\"linear\"``, ``\"poly\"``, ``\"polynomial\"``, ``\"rbf\"``, ``\"laplacian\"``, ``\"sigmoid\"``, ``\"cosine\"``] References ---------- .. footbibliography:: \"\"\" if isinstance ( kernel_x , str ) and kernel_x not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are {PAIRWISE_KERNEL_FUNCTIONS}. \" f \"You passed in {kernel_x} for kernel_x.\" ) if isinstance ( kernel_y , str ) and kernel_y not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are {PAIRWISE_KERNEL_FUNCTIONS}. \" f \"You passed in {kernel_y} for kernel_y.\" ) if isinstance ( kernel_z , str ) and kernel_z not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are {PAIRWISE_KERNEL_FUNCTIONS}. \" f \"You passed in {kernel_z} for kernel_z.\" ) self . kernel_x = kernel_x self . kernel_y = kernel_y self . kernel_z = kernel_z self . null_size = null_size self . approx_with_gamma = approx_with_gamma self . threshold = threshold self . n_jobs = n_jobs # hyperparameters of the kernsl self . kwidth_x = kwidth_x self . kwidth_y = kwidth_y self . kwidth_z = kwidth_z def test ( self , df , x_var , y_var , z_covariates = None ): self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None or len ( z_covariates ) == 0 : Z = None else : z_covariates = list ( z_covariates ) Z = df [ z_covariates ] . to_numpy () . reshape (( - 1 , len ( z_covariates ))) X = df [ x_var ] . to_numpy ()[:, np . newaxis ] Y = df [ y_var ] . to_numpy ()[:, np . newaxis ] # first normalize the data to have zero mean and unit variance # along the columns of the data X = stats . zscore ( X , axis = 0 ) Y = stats . zscore ( Y , axis = 0 ) if Z is not None : Z = stats . zscore ( Z , axis = 0 ) # when running CI, \\ddot{X} comprises of (X, Z) X = np . concatenate (( X , Z ), axis = 1 ) Kz , sigma_z = self . _compute_kernel ( Z , distance_metric = \"l2\" , metric = self . kernel_z , kwidth = self . kwidth_z , centered = True , ) # compute the centralized kernel matrices of each the datasets Kx , sigma_x = self . _compute_kernel ( X , distance_metric = \"l2\" , metric = self . kernel_x , kwidth = self . kwidth_x , centered = True , ) Ky , sigma_y = self . _compute_kernel ( Y , distance_metric = \"l2\" , metric = self . kernel_y , kwidth = self . kwidth_y , centered = True ) if Z is None : # test statistic is just the normal bivariate independence # test statistic test_stat = self . _compute_V_statistic ( Kx , Ky ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ind ( Kx , Ky ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ind ( Kx , Ky , n_samples = self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) else : # compute the centralizing matrix for the kernels according to # conditioning set Z epsilon = 1e-7 n = Kx . shape [ 0 ] Rz = epsilon * np . linalg . pinv ( Kz + epsilon * np . eye ( n )) # compute the centralized kernel matrices KxzR = Rz . dot ( Kx ) . dot ( Rz ) KyzR = Rz . dot ( Ky ) . dot ( Rz ) # compute the conditional independence test statistic test_stat = self . _compute_V_statistic ( KxzR , KyzR ) # compute the product of the eigenvectors uu_prod = self . _compute_prod_eigvecs ( KxzR , KyzR , threshold = self . threshold ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ci ( uu_prod ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ci ( uu_prod , self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) return test_stat , pvalue def _approx_gamma_params_ind ( self , Kx , Ky ): T = Kx . shape [ 0 ] mean_appr = np . trace ( Kx ) * np . trace ( Ky ) / T var_appr = 2 * np . trace ( Kx . dot ( Kx )) * np . trace ( Ky . dot ( Ky )) / T / T k_appr = mean_appr ** 2 / var_appr theta_appr = var_appr / mean_appr return k_appr , theta_appr def _approx_gamma_params_ci ( self , uu_prod ): \"\"\"Get parameters of the approximated Gamma distribution. Parameters ---------- uu_prod : np.ndarray of shape (n_features, n_features) The product of the eigenvectors of Kx and Ky, the kernels on the input data, X and Y. Returns ------- k_appr : float The shape parameter of the Gamma distribution. theta_appr : float The scale parameter of the Gamma distribution. Notes ----- X ~ Gamma(k, theta) with a probability density function of the following: .. math:: f(x; k, \\\\theta) = \\\\frac{x^{k-1} e^{-x / \\\\theta}}{\\\\theta^k \\\\Gamma(k)} where $\\\\Gamma(k)$ is the Gamma function evaluated at k. In this scenario k governs the shape of the pdf, while $\\\\theta$ governs more how spread out the data is. \"\"\" # approximate the mean and the variance mean_appr = np . trace ( uu_prod ) var_appr = 2 * np . trace ( uu_prod . dot ( uu_prod )) k_appr = mean_appr ** 2 / var_appr theta_appr = var_appr / mean_appr return k_appr , theta_appr def _compute_prod_eigvecs ( self , Kx , Ky , threshold = None ): T = Kx . shape [ 0 ] wx , vx = np . linalg . eigh ( 0.5 * ( Kx + Kx . T )) wy , vy = np . linalg . eigh ( 0.5 * ( Ky + Ky . T )) if threshold is not None : # threshold eigenvalues that are below a certain threshold # and remove their corresponding values and eigenvectors vx = vx [:, wx > np . max ( wx ) * threshold ] wx = wx [ wx > np . max ( wx ) * threshold ] vy = vy [:, wy > np . max ( wy ) * threshold ] wy = wy [ wy > np . max ( wy ) * threshold ] # scale the eigenvectors by their eigenvalues vx = vx . dot ( np . diag ( np . sqrt ( wx ))) vy = vy . dot ( np . diag ( np . sqrt ( wy ))) # compute the product of the scaled eigenvectors num_eigx = vx . shape [ 1 ] num_eigy = vy . shape [ 1 ] size_u = num_eigx * num_eigy uu = np . zeros (( T , size_u )) for i in range ( 0 , num_eigx ): for j in range ( 0 , num_eigy ): # compute the dot product of eigenvectors uu [:, i * num_eigy + j ] = vx [:, i ] * vy [:, j ] # now compute the product if size_u > T : uu_prod = uu . dot ( uu . T ) else : uu_prod = uu . T . dot ( uu ) return uu_prod def _compute_V_statistic ( self , KxR , KyR ): # n = KxR.shape[0] # compute the sum of the two kernsl Vstat = np . sum ( KxR * KyR ) return Vstat def _compute_null_ind ( self , Kx , Ky , n_samples , max_num_eigs = 1000 ): n = Kx . shape [ 0 ] # get the eigenvalues in ascending order, smallest to largest eigvals_x = np . linalg . eigvalsh ( Kx ) eigvals_y = np . linalg . eigvalsh ( Ky ) # optionally only keep the largest \"N\" eigenvalues eigvals_x = eigvals_x [ - max_num_eigs :] eigvals_y = eigvals_y [ - max_num_eigs :] num_eigs = len ( eigvals_x ) # compute the entry-wise product of the eigenvalues and store it as a vector eigvals_prod = np . dot ( eigvals_x . reshape ( num_eigs , 1 ), eigvals_y . reshape ( 1 , num_eigs ) ) . reshape (( - 1 , 1 )) # only keep eigenvalues above a certain threshold eigvals_prod = eigvals_prod [ eigvals_prod > eigvals_prod . max () * self . threshold ] # generate chi-square distributed values z_{ij} with degree of freedom 1 f_rand = np . random . chisquare ( df = 1 , size = ( len ( eigvals_prod ), n_samples )) # compute the null distribution consisting now of (n_samples) # of chi-squared random variables weighted by the eigenvalue products null_dist = 1.0 / n * eigvals_prod . T . dot ( f_rand ) return null_dist def _compute_null_ci ( self , uu_prod , n_samples ): # the eigenvalues of ww^T eig_uu = np . linalg . eigvalsh ( uu_prod ) eig_uu = eig_uu [ eig_uu > eig_uu . max () * self . threshold ] # generate chi-square distributed values z_{ij} with degree of freedom 1 f_rand = np . random . chisquare ( df = 1 , size = ( eig_uu . shape [ 0 ], n_samples )) # compute the null distribution consisting now of (n_samples) # of chi-squared random variables weighted by the eigenvalue products null_dist = eig_uu . T . dot ( f_rand ) return null_dist def _compute_kernel ( self , X , Y = None , distance_metric = \"l2\" , metric = \"rbf\" , kwidth = None , centered = True ): # if the width of the kernel is not set, then use the median trick to set the # kernel width based on the data X if kwidth is None : # Note: sigma = 1 / np.sqrt(kwidth) # compute N x N pairwise distance matrix dists = pairwise_distances ( X , metric = distance_metric , n_jobs = self . n_jobs ) # compute median of off diagonal elements med = np . median ( dists [ dists > 0 ]) # prevents division by zero when used on label vectors med = med if med else 1 else : med = kwidth extra_kwargs = dict () if metric == \"rbf\" : # compute the normalization factor of the width of the Gaussian kernel gamma = 1.0 / ( 2 * ( med ** 2 )) extra_kwargs [ \"gamma\" ] = gamma elif metric == \"polynomial\" : degree = 2 extra_kwargs [ \"degree\" ] = degree # compute the potentially pairwise kernel kernel = pairwise_kernels ( X , Y = Y , metric = metric , n_jobs = self . n_jobs , ** extra_kwargs ) if centered : kernel = self . _center_kernel ( kernel ) return kernel , med def _center_kernel ( self , K ): \"\"\"Centers the kernel matrix. Applies a transformation H * K * H, where H is a diagonal matrix with 1/n along the diagonal. \"\"\" n = K . shape [ 0 ] H = np . eye ( n ) - 1.0 / n return H . dot ( K ) . dot ( H ) Ancestors (in MRO) dodiscover.ci.base.BaseConditionalIndependenceTest Methods test def test ( self , df , x_var , y_var , z_covariates = None ) Abstract method for all conditional independence tests. Parameters: Name Type Description Default df pd.DataFrame description None x_var Any description None y_var Any description None z_covariates Any description , by default None None Returns: Type Description Tuple[float, float] description View Source def test ( self , df , x_var , y_var , z_covariates = None ) : self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None or len ( z_covariates ) == 0 : Z = None else : z_covariates = list ( z_covariates ) Z = df [ z_covariates ] . to_numpy (). reshape (( - 1 , len ( z_covariates ))) X = df [ x_var ] . to_numpy () [ :, np.newaxis ] Y = df [ y_var ] . to_numpy () [ :, np.newaxis ] # first normalize the data to have zero mean and unit variance # along the columns of the data X = stats . zscore ( X , axis = 0 ) Y = stats . zscore ( Y , axis = 0 ) if Z is not None : Z = stats . zscore ( Z , axis = 0 ) # when running CI , \\ ddot { X } comprises of ( X , Z ) X = np . concatenate (( X , Z ), axis = 1 ) Kz , sigma_z = self . _compute_kernel ( Z , distance_metric = \"l2\" , metric = self . kernel_z , kwidth = self . kwidth_z , centered = True , ) # compute the centralized kernel matrices of each the datasets Kx , sigma_x = self . _compute_kernel ( X , distance_metric = \"l2\" , metric = self . kernel_x , kwidth = self . kwidth_x , centered = True , ) Ky , sigma_y = self . _compute_kernel ( Y , distance_metric = \"l2\" , metric = self . kernel_y , kwidth = self . kwidth_y , centered = True ) if Z is None : # test statistic is just the normal bivariate independence # test statistic test_stat = self . _compute_V_statistic ( Kx , Ky ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ind ( Kx , Ky ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ind ( Kx , Ky , n_samples = self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) else : # compute the centralizing matrix for the kernels according to # conditioning set Z epsilon = 1e-7 n = Kx . shape [ 0 ] Rz = epsilon * np . linalg . pinv ( Kz + epsilon * np . eye ( n )) # compute the centralized kernel matrices KxzR = Rz . dot ( Kx ). dot ( Rz ) KyzR = Rz . dot ( Ky ). dot ( Rz ) # compute the conditional independence test statistic test_stat = self . _compute_V_statistic ( KxzR , KyzR ) # compute the product of the eigenvectors uu_prod = self . _compute_prod_eigvecs ( KxzR , KyzR , threshold = self . threshold ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ci ( uu_prod ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ci ( uu_prod , self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) return test_stat , pvalue","title":"Kernel Test"},{"location":"reference/dodiscover/ci/kernel_test/#module-dodiscovercikernel_test","text":"None None View Source import numpy as np from scipy import stats from sklearn.metrics import pairwise_distances , pairwise_kernels from sklearn.metrics.pairwise import PAIRWISE_KERNEL_FUNCTIONS from .base import BaseConditionalIndependenceTest class KernelCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , kernel_x : str = \"rbf\" , kernel_y : str = \"rbf\" , kernel_z : str = \"rbf\" , null_size : int = 1000 , approx_with_gamma : bool = True , kwidth_x = None , kwidth_y = None , kwidth_z = None , threshold : float = 1e-5 , n_jobs : int = None , ): \"\"\"Kernel (Conditional) Independence Test. For testing (conditional) independence on continuous data, we leverage kernels :footcite:`Zhang2011` that are computationally efficient. Parameters ---------- kernel_x : str, optional The kernel function for data 'X', by default \"rbf\". kernel_y : str, optional The kernel function for data 'Y', by default \"rbf\". kernel_z : str, optional The kernel function for data 'Z', by default \"rbf\". null_size : int, optional The number of samples to generate for the bootstrap distribution to approximate the pvalue, by default 1000. approx_with_gamma : bool, optional Whether to use the Gamma distribution approximation for the pvalue, by default True. kwidth_x : _type_, optional _description_, by default None kwidth_y : _type_, optional _description_, by default None kwidth_z : _type_, optional _description_, by default None threshold : float, optional The threshold set on the value of eigenvalues, by default 1e-5. Used to regularize the method. n_jobs : int, optional The number of CPUs to use, by default None. Notes ----- Valid strings for ``compute_kernel`` are, as defined in :func:`sklearn.metrics.pairwise.pairwise_kernels`, [``\"additive_chi2\"``, ``\"chi2\"``, ``\"linear\"``, ``\"poly\"``, ``\"polynomial\"``, ``\"rbf\"``, ``\"laplacian\"``, ``\"sigmoid\"``, ``\"cosine\"``] References ---------- .. footbibliography:: \"\"\" if isinstance ( kernel_x , str ) and kernel_x not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are { PAIRWISE_KERNEL_FUNCTIONS } . \" f \"You passed in { kernel_x } for kernel_x.\" ) if isinstance ( kernel_y , str ) and kernel_y not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are { PAIRWISE_KERNEL_FUNCTIONS } . \" f \"You passed in { kernel_y } for kernel_y.\" ) if isinstance ( kernel_z , str ) and kernel_z not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are { PAIRWISE_KERNEL_FUNCTIONS } . \" f \"You passed in { kernel_z } for kernel_z.\" ) self . kernel_x = kernel_x self . kernel_y = kernel_y self . kernel_z = kernel_z self . null_size = null_size self . approx_with_gamma = approx_with_gamma self . threshold = threshold self . n_jobs = n_jobs # hyperparameters of the kernsl self . kwidth_x = kwidth_x self . kwidth_y = kwidth_y self . kwidth_z = kwidth_z def test ( self , df , x_var , y_var , z_covariates = None ): self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None or len ( z_covariates ) == 0 : Z = None else : z_covariates = list ( z_covariates ) Z = df [ z_covariates ] . to_numpy () . reshape (( - 1 , len ( z_covariates ))) X = df [ x_var ] . to_numpy ()[:, np . newaxis ] Y = df [ y_var ] . to_numpy ()[:, np . newaxis ] # first normalize the data to have zero mean and unit variance # along the columns of the data X = stats . zscore ( X , axis = 0 ) Y = stats . zscore ( Y , axis = 0 ) if Z is not None : Z = stats . zscore ( Z , axis = 0 ) # when running CI, \\ddot{X} comprises of (X, Z) X = np . concatenate (( X , Z ), axis = 1 ) Kz , sigma_z = self . _compute_kernel ( Z , distance_metric = \"l2\" , metric = self . kernel_z , kwidth = self . kwidth_z , centered = True , ) # compute the centralized kernel matrices of each the datasets Kx , sigma_x = self . _compute_kernel ( X , distance_metric = \"l2\" , metric = self . kernel_x , kwidth = self . kwidth_x , centered = True , ) Ky , sigma_y = self . _compute_kernel ( Y , distance_metric = \"l2\" , metric = self . kernel_y , kwidth = self . kwidth_y , centered = True ) if Z is None : # test statistic is just the normal bivariate independence # test statistic test_stat = self . _compute_V_statistic ( Kx , Ky ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ind ( Kx , Ky ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ind ( Kx , Ky , n_samples = self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) else : # compute the centralizing matrix for the kernels according to # conditioning set Z epsilon = 1e-7 n = Kx . shape [ 0 ] Rz = epsilon * np . linalg . pinv ( Kz + epsilon * np . eye ( n )) # compute the centralized kernel matrices KxzR = Rz . dot ( Kx ) . dot ( Rz ) KyzR = Rz . dot ( Ky ) . dot ( Rz ) # compute the conditional independence test statistic test_stat = self . _compute_V_statistic ( KxzR , KyzR ) # compute the product of the eigenvectors uu_prod = self . _compute_prod_eigvecs ( KxzR , KyzR , threshold = self . threshold ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ci ( uu_prod ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ci ( uu_prod , self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) return test_stat , pvalue def _approx_gamma_params_ind ( self , Kx , Ky ): T = Kx . shape [ 0 ] mean_appr = np . trace ( Kx ) * np . trace ( Ky ) / T var_appr = 2 * np . trace ( Kx . dot ( Kx )) * np . trace ( Ky . dot ( Ky )) / T / T k_appr = mean_appr ** 2 / var_appr theta_appr = var_appr / mean_appr return k_appr , theta_appr def _approx_gamma_params_ci ( self , uu_prod ): \"\"\"Get parameters of the approximated Gamma distribution. Parameters ---------- uu_prod : np.ndarray of shape (n_features, n_features) The product of the eigenvectors of Kx and Ky, the kernels on the input data, X and Y. Returns ------- k_appr : float The shape parameter of the Gamma distribution. theta_appr : float The scale parameter of the Gamma distribution. Notes ----- X ~ Gamma(k, theta) with a probability density function of the following: .. math:: f(x; k, \\\\theta) = \\\\frac{x^{k-1} e^{-x / \\\\theta}}{\\\\theta^k \\\\Gamma(k)} where $\\\\Gamma(k)$ is the Gamma function evaluated at k. In this scenario k governs the shape of the pdf, while $\\\\theta$ governs more how spread out the data is. \"\"\" # approximate the mean and the variance mean_appr = np . trace ( uu_prod ) var_appr = 2 * np . trace ( uu_prod . dot ( uu_prod )) k_appr = mean_appr ** 2 / var_appr theta_appr = var_appr / mean_appr return k_appr , theta_appr def _compute_prod_eigvecs ( self , Kx , Ky , threshold = None ): T = Kx . shape [ 0 ] wx , vx = np . linalg . eigh ( 0.5 * ( Kx + Kx . T )) wy , vy = np . linalg . eigh ( 0.5 * ( Ky + Ky . T )) if threshold is not None : # threshold eigenvalues that are below a certain threshold # and remove their corresponding values and eigenvectors vx = vx [:, wx > np . max ( wx ) * threshold ] wx = wx [ wx > np . max ( wx ) * threshold ] vy = vy [:, wy > np . max ( wy ) * threshold ] wy = wy [ wy > np . max ( wy ) * threshold ] # scale the eigenvectors by their eigenvalues vx = vx . dot ( np . diag ( np . sqrt ( wx ))) vy = vy . dot ( np . diag ( np . sqrt ( wy ))) # compute the product of the scaled eigenvectors num_eigx = vx . shape [ 1 ] num_eigy = vy . shape [ 1 ] size_u = num_eigx * num_eigy uu = np . zeros (( T , size_u )) for i in range ( 0 , num_eigx ): for j in range ( 0 , num_eigy ): # compute the dot product of eigenvectors uu [:, i * num_eigy + j ] = vx [:, i ] * vy [:, j ] # now compute the product if size_u > T : uu_prod = uu . dot ( uu . T ) else : uu_prod = uu . T . dot ( uu ) return uu_prod def _compute_V_statistic ( self , KxR , KyR ): # n = KxR.shape[0] # compute the sum of the two kernsl Vstat = np . sum ( KxR * KyR ) return Vstat def _compute_null_ind ( self , Kx , Ky , n_samples , max_num_eigs = 1000 ): n = Kx . shape [ 0 ] # get the eigenvalues in ascending order, smallest to largest eigvals_x = np . linalg . eigvalsh ( Kx ) eigvals_y = np . linalg . eigvalsh ( Ky ) # optionally only keep the largest \"N\" eigenvalues eigvals_x = eigvals_x [ - max_num_eigs :] eigvals_y = eigvals_y [ - max_num_eigs :] num_eigs = len ( eigvals_x ) # compute the entry-wise product of the eigenvalues and store it as a vector eigvals_prod = np . dot ( eigvals_x . reshape ( num_eigs , 1 ), eigvals_y . reshape ( 1 , num_eigs ) ) . reshape (( - 1 , 1 )) # only keep eigenvalues above a certain threshold eigvals_prod = eigvals_prod [ eigvals_prod > eigvals_prod . max () * self . threshold ] # generate chi-square distributed values z_{ij} with degree of freedom 1 f_rand = np . random . chisquare ( df = 1 , size = ( len ( eigvals_prod ), n_samples )) # compute the null distribution consisting now of (n_samples) # of chi-squared random variables weighted by the eigenvalue products null_dist = 1.0 / n * eigvals_prod . T . dot ( f_rand ) return null_dist def _compute_null_ci ( self , uu_prod , n_samples ): # the eigenvalues of ww^T eig_uu = np . linalg . eigvalsh ( uu_prod ) eig_uu = eig_uu [ eig_uu > eig_uu . max () * self . threshold ] # generate chi-square distributed values z_{ij} with degree of freedom 1 f_rand = np . random . chisquare ( df = 1 , size = ( eig_uu . shape [ 0 ], n_samples )) # compute the null distribution consisting now of (n_samples) # of chi-squared random variables weighted by the eigenvalue products null_dist = eig_uu . T . dot ( f_rand ) return null_dist def _compute_kernel ( self , X , Y = None , distance_metric = \"l2\" , metric = \"rbf\" , kwidth = None , centered = True ): # if the width of the kernel is not set, then use the median trick to set the # kernel width based on the data X if kwidth is None : # Note: sigma = 1 / np.sqrt(kwidth) # compute N x N pairwise distance matrix dists = pairwise_distances ( X , metric = distance_metric , n_jobs = self . n_jobs ) # compute median of off diagonal elements med = np . median ( dists [ dists > 0 ]) # prevents division by zero when used on label vectors med = med if med else 1 else : med = kwidth extra_kwargs = dict () if metric == \"rbf\" : # compute the normalization factor of the width of the Gaussian kernel gamma = 1.0 / ( 2 * ( med ** 2 )) extra_kwargs [ \"gamma\" ] = gamma elif metric == \"polynomial\" : degree = 2 extra_kwargs [ \"degree\" ] = degree # compute the potentially pairwise kernel kernel = pairwise_kernels ( X , Y = Y , metric = metric , n_jobs = self . n_jobs , ** extra_kwargs ) if centered : kernel = self . _center_kernel ( kernel ) return kernel , med def _center_kernel ( self , K ): \"\"\"Centers the kernel matrix. Applies a transformation H * K * H, where H is a diagonal matrix with 1/n along the diagonal. \"\"\" n = K . shape [ 0 ] H = np . eye ( n ) - 1.0 / n return H . dot ( K ) . dot ( H )","title":"Module dodiscover.ci.kernel_test"},{"location":"reference/dodiscover/ci/kernel_test/#variables","text":"PAIRWISE_KERNEL_FUNCTIONS","title":"Variables"},{"location":"reference/dodiscover/ci/kernel_test/#classes","text":"","title":"Classes"},{"location":"reference/dodiscover/ci/kernel_test/#kernelcitest","text":"class KernelCITest ( kernel_x : str = 'rbf' , kernel_y : str = 'rbf' , kernel_z : str = 'rbf' , null_size : int = 1000 , approx_with_gamma : bool = True , kwidth_x = None , kwidth_y = None , kwidth_z = None , threshold : float = 1e-05 , n_jobs : int = None ) View Source class KernelCITest ( BaseConditionalIndependenceTest ): def __init__ ( self , kernel_x : str = \"rbf\" , kernel_y : str = \"rbf\" , kernel_z : str = \"rbf\" , null_size : int = 1000 , approx_with_gamma : bool = True , kwidth_x = None , kwidth_y = None , kwidth_z = None , threshold : float = 1e-5 , n_jobs : int = None , ): \"\"\"Kernel (Conditional) Independence Test. For testing (conditional) independence on continuous data, we leverage kernels :footcite:`Zhang2011` that are computationally efficient. Parameters ---------- kernel_x : str, optional The kernel function for data 'X', by default \"rbf\". kernel_y : str, optional The kernel function for data 'Y', by default \"rbf\". kernel_z : str, optional The kernel function for data 'Z', by default \"rbf\". null_size : int, optional The number of samples to generate for the bootstrap distribution to approximate the pvalue, by default 1000. approx_with_gamma : bool, optional Whether to use the Gamma distribution approximation for the pvalue, by default True. kwidth_x : _type_, optional _description_, by default None kwidth_y : _type_, optional _description_, by default None kwidth_z : _type_, optional _description_, by default None threshold : float, optional The threshold set on the value of eigenvalues, by default 1e-5. Used to regularize the method. n_jobs : int, optional The number of CPUs to use, by default None. Notes ----- Valid strings for ``compute_kernel`` are, as defined in :func:`sklearn.metrics.pairwise.pairwise_kernels`, [``\"additive_chi2\"``, ``\"chi2\"``, ``\"linear\"``, ``\"poly\"``, ``\"polynomial\"``, ``\"rbf\"``, ``\"laplacian\"``, ``\"sigmoid\"``, ``\"cosine\"``] References ---------- .. footbibliography:: \"\"\" if isinstance ( kernel_x , str ) and kernel_x not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are {PAIRWISE_KERNEL_FUNCTIONS}. \" f \"You passed in {kernel_x} for kernel_x.\" ) if isinstance ( kernel_y , str ) and kernel_y not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are {PAIRWISE_KERNEL_FUNCTIONS}. \" f \"You passed in {kernel_y} for kernel_y.\" ) if isinstance ( kernel_z , str ) and kernel_z not in PAIRWISE_KERNEL_FUNCTIONS : raise ValueError ( f \"The kernels that are currently supported are {PAIRWISE_KERNEL_FUNCTIONS}. \" f \"You passed in {kernel_z} for kernel_z.\" ) self . kernel_x = kernel_x self . kernel_y = kernel_y self . kernel_z = kernel_z self . null_size = null_size self . approx_with_gamma = approx_with_gamma self . threshold = threshold self . n_jobs = n_jobs # hyperparameters of the kernsl self . kwidth_x = kwidth_x self . kwidth_y = kwidth_y self . kwidth_z = kwidth_z def test ( self , df , x_var , y_var , z_covariates = None ): self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None or len ( z_covariates ) == 0 : Z = None else : z_covariates = list ( z_covariates ) Z = df [ z_covariates ] . to_numpy () . reshape (( - 1 , len ( z_covariates ))) X = df [ x_var ] . to_numpy ()[:, np . newaxis ] Y = df [ y_var ] . to_numpy ()[:, np . newaxis ] # first normalize the data to have zero mean and unit variance # along the columns of the data X = stats . zscore ( X , axis = 0 ) Y = stats . zscore ( Y , axis = 0 ) if Z is not None : Z = stats . zscore ( Z , axis = 0 ) # when running CI, \\ddot{X} comprises of (X, Z) X = np . concatenate (( X , Z ), axis = 1 ) Kz , sigma_z = self . _compute_kernel ( Z , distance_metric = \"l2\" , metric = self . kernel_z , kwidth = self . kwidth_z , centered = True , ) # compute the centralized kernel matrices of each the datasets Kx , sigma_x = self . _compute_kernel ( X , distance_metric = \"l2\" , metric = self . kernel_x , kwidth = self . kwidth_x , centered = True , ) Ky , sigma_y = self . _compute_kernel ( Y , distance_metric = \"l2\" , metric = self . kernel_y , kwidth = self . kwidth_y , centered = True ) if Z is None : # test statistic is just the normal bivariate independence # test statistic test_stat = self . _compute_V_statistic ( Kx , Ky ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ind ( Kx , Ky ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ind ( Kx , Ky , n_samples = self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) else : # compute the centralizing matrix for the kernels according to # conditioning set Z epsilon = 1e-7 n = Kx . shape [ 0 ] Rz = epsilon * np . linalg . pinv ( Kz + epsilon * np . eye ( n )) # compute the centralized kernel matrices KxzR = Rz . dot ( Kx ) . dot ( Rz ) KyzR = Rz . dot ( Ky ) . dot ( Rz ) # compute the conditional independence test statistic test_stat = self . _compute_V_statistic ( KxzR , KyzR ) # compute the product of the eigenvectors uu_prod = self . _compute_prod_eigvecs ( KxzR , KyzR , threshold = self . threshold ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ci ( uu_prod ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ci ( uu_prod , self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) return test_stat , pvalue def _approx_gamma_params_ind ( self , Kx , Ky ): T = Kx . shape [ 0 ] mean_appr = np . trace ( Kx ) * np . trace ( Ky ) / T var_appr = 2 * np . trace ( Kx . dot ( Kx )) * np . trace ( Ky . dot ( Ky )) / T / T k_appr = mean_appr ** 2 / var_appr theta_appr = var_appr / mean_appr return k_appr , theta_appr def _approx_gamma_params_ci ( self , uu_prod ): \"\"\"Get parameters of the approximated Gamma distribution. Parameters ---------- uu_prod : np.ndarray of shape (n_features, n_features) The product of the eigenvectors of Kx and Ky, the kernels on the input data, X and Y. Returns ------- k_appr : float The shape parameter of the Gamma distribution. theta_appr : float The scale parameter of the Gamma distribution. Notes ----- X ~ Gamma(k, theta) with a probability density function of the following: .. math:: f(x; k, \\\\theta) = \\\\frac{x^{k-1} e^{-x / \\\\theta}}{\\\\theta^k \\\\Gamma(k)} where $\\\\Gamma(k)$ is the Gamma function evaluated at k. In this scenario k governs the shape of the pdf, while $\\\\theta$ governs more how spread out the data is. \"\"\" # approximate the mean and the variance mean_appr = np . trace ( uu_prod ) var_appr = 2 * np . trace ( uu_prod . dot ( uu_prod )) k_appr = mean_appr ** 2 / var_appr theta_appr = var_appr / mean_appr return k_appr , theta_appr def _compute_prod_eigvecs ( self , Kx , Ky , threshold = None ): T = Kx . shape [ 0 ] wx , vx = np . linalg . eigh ( 0.5 * ( Kx + Kx . T )) wy , vy = np . linalg . eigh ( 0.5 * ( Ky + Ky . T )) if threshold is not None : # threshold eigenvalues that are below a certain threshold # and remove their corresponding values and eigenvectors vx = vx [:, wx > np . max ( wx ) * threshold ] wx = wx [ wx > np . max ( wx ) * threshold ] vy = vy [:, wy > np . max ( wy ) * threshold ] wy = wy [ wy > np . max ( wy ) * threshold ] # scale the eigenvectors by their eigenvalues vx = vx . dot ( np . diag ( np . sqrt ( wx ))) vy = vy . dot ( np . diag ( np . sqrt ( wy ))) # compute the product of the scaled eigenvectors num_eigx = vx . shape [ 1 ] num_eigy = vy . shape [ 1 ] size_u = num_eigx * num_eigy uu = np . zeros (( T , size_u )) for i in range ( 0 , num_eigx ): for j in range ( 0 , num_eigy ): # compute the dot product of eigenvectors uu [:, i * num_eigy + j ] = vx [:, i ] * vy [:, j ] # now compute the product if size_u > T : uu_prod = uu . dot ( uu . T ) else : uu_prod = uu . T . dot ( uu ) return uu_prod def _compute_V_statistic ( self , KxR , KyR ): # n = KxR.shape[0] # compute the sum of the two kernsl Vstat = np . sum ( KxR * KyR ) return Vstat def _compute_null_ind ( self , Kx , Ky , n_samples , max_num_eigs = 1000 ): n = Kx . shape [ 0 ] # get the eigenvalues in ascending order, smallest to largest eigvals_x = np . linalg . eigvalsh ( Kx ) eigvals_y = np . linalg . eigvalsh ( Ky ) # optionally only keep the largest \"N\" eigenvalues eigvals_x = eigvals_x [ - max_num_eigs :] eigvals_y = eigvals_y [ - max_num_eigs :] num_eigs = len ( eigvals_x ) # compute the entry-wise product of the eigenvalues and store it as a vector eigvals_prod = np . dot ( eigvals_x . reshape ( num_eigs , 1 ), eigvals_y . reshape ( 1 , num_eigs ) ) . reshape (( - 1 , 1 )) # only keep eigenvalues above a certain threshold eigvals_prod = eigvals_prod [ eigvals_prod > eigvals_prod . max () * self . threshold ] # generate chi-square distributed values z_{ij} with degree of freedom 1 f_rand = np . random . chisquare ( df = 1 , size = ( len ( eigvals_prod ), n_samples )) # compute the null distribution consisting now of (n_samples) # of chi-squared random variables weighted by the eigenvalue products null_dist = 1.0 / n * eigvals_prod . T . dot ( f_rand ) return null_dist def _compute_null_ci ( self , uu_prod , n_samples ): # the eigenvalues of ww^T eig_uu = np . linalg . eigvalsh ( uu_prod ) eig_uu = eig_uu [ eig_uu > eig_uu . max () * self . threshold ] # generate chi-square distributed values z_{ij} with degree of freedom 1 f_rand = np . random . chisquare ( df = 1 , size = ( eig_uu . shape [ 0 ], n_samples )) # compute the null distribution consisting now of (n_samples) # of chi-squared random variables weighted by the eigenvalue products null_dist = eig_uu . T . dot ( f_rand ) return null_dist def _compute_kernel ( self , X , Y = None , distance_metric = \"l2\" , metric = \"rbf\" , kwidth = None , centered = True ): # if the width of the kernel is not set, then use the median trick to set the # kernel width based on the data X if kwidth is None : # Note: sigma = 1 / np.sqrt(kwidth) # compute N x N pairwise distance matrix dists = pairwise_distances ( X , metric = distance_metric , n_jobs = self . n_jobs ) # compute median of off diagonal elements med = np . median ( dists [ dists > 0 ]) # prevents division by zero when used on label vectors med = med if med else 1 else : med = kwidth extra_kwargs = dict () if metric == \"rbf\" : # compute the normalization factor of the width of the Gaussian kernel gamma = 1.0 / ( 2 * ( med ** 2 )) extra_kwargs [ \"gamma\" ] = gamma elif metric == \"polynomial\" : degree = 2 extra_kwargs [ \"degree\" ] = degree # compute the potentially pairwise kernel kernel = pairwise_kernels ( X , Y = Y , metric = metric , n_jobs = self . n_jobs , ** extra_kwargs ) if centered : kernel = self . _center_kernel ( kernel ) return kernel , med def _center_kernel ( self , K ): \"\"\"Centers the kernel matrix. Applies a transformation H * K * H, where H is a diagonal matrix with 1/n along the diagonal. \"\"\" n = K . shape [ 0 ] H = np . eye ( n ) - 1.0 / n return H . dot ( K ) . dot ( H )","title":"KernelCITest"},{"location":"reference/dodiscover/ci/kernel_test/#ancestors-in-mro","text":"dodiscover.ci.base.BaseConditionalIndependenceTest","title":"Ancestors (in MRO)"},{"location":"reference/dodiscover/ci/kernel_test/#methods","text":"","title":"Methods"},{"location":"reference/dodiscover/ci/kernel_test/#test","text":"def test ( self , df , x_var , y_var , z_covariates = None ) Abstract method for all conditional independence tests. Parameters: Name Type Description Default df pd.DataFrame description None x_var Any description None y_var Any description None z_covariates Any description , by default None None Returns: Type Description Tuple[float, float] description View Source def test ( self , df , x_var , y_var , z_covariates = None ) : self . _check_test_input ( df , x_var , y_var , z_covariates ) if z_covariates is None or len ( z_covariates ) == 0 : Z = None else : z_covariates = list ( z_covariates ) Z = df [ z_covariates ] . to_numpy (). reshape (( - 1 , len ( z_covariates ))) X = df [ x_var ] . to_numpy () [ :, np.newaxis ] Y = df [ y_var ] . to_numpy () [ :, np.newaxis ] # first normalize the data to have zero mean and unit variance # along the columns of the data X = stats . zscore ( X , axis = 0 ) Y = stats . zscore ( Y , axis = 0 ) if Z is not None : Z = stats . zscore ( Z , axis = 0 ) # when running CI , \\ ddot { X } comprises of ( X , Z ) X = np . concatenate (( X , Z ), axis = 1 ) Kz , sigma_z = self . _compute_kernel ( Z , distance_metric = \"l2\" , metric = self . kernel_z , kwidth = self . kwidth_z , centered = True , ) # compute the centralized kernel matrices of each the datasets Kx , sigma_x = self . _compute_kernel ( X , distance_metric = \"l2\" , metric = self . kernel_x , kwidth = self . kwidth_x , centered = True , ) Ky , sigma_y = self . _compute_kernel ( Y , distance_metric = \"l2\" , metric = self . kernel_y , kwidth = self . kwidth_y , centered = True ) if Z is None : # test statistic is just the normal bivariate independence # test statistic test_stat = self . _compute_V_statistic ( Kx , Ky ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ind ( Kx , Ky ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ind ( Kx , Ky , n_samples = self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) else : # compute the centralizing matrix for the kernels according to # conditioning set Z epsilon = 1e-7 n = Kx . shape [ 0 ] Rz = epsilon * np . linalg . pinv ( Kz + epsilon * np . eye ( n )) # compute the centralized kernel matrices KxzR = Rz . dot ( Kx ). dot ( Rz ) KyzR = Rz . dot ( Ky ). dot ( Rz ) # compute the conditional independence test statistic test_stat = self . _compute_V_statistic ( KxzR , KyzR ) # compute the product of the eigenvectors uu_prod = self . _compute_prod_eigvecs ( KxzR , KyzR , threshold = self . threshold ) if self . approx_with_gamma : # approximate the pvalue using the Gamma distribution k_appr , theta_appr = self . _approx_gamma_params_ci ( uu_prod ) pvalue = 1 - stats . gamma . cdf ( test_stat , k_appr , 0 , theta_appr ) else : null_samples = self . _compute_null_ci ( uu_prod , self . null_size ) pvalue = np . sum ( null_samples > test_stat ) / float ( self . null_size ) return test_stat , pvalue","title":"test"},{"location":"reference/dodiscover/ci/oracle/","text":"Module dodiscover.ci.oracle None None View Source from typing import Union import networkx as nx import numpy as np from graphs.algorithms import m_separated from networkx.algorithms import d_separated from pywhy_graphs import ADMG from .base import BaseConditionalIndependenceTest class Oracle ( BaseConditionalIndependenceTest ): \"\"\"Oracle conditional independence testing. Used for unit testing and checking intuition. Parameters ---------- graph : nx.DiGraph | pywhy_graphs.ADMG The ground-truth causal graph. \"\"\" def __init__ ( self , graph : Union [ ADMG , nx . DiGraph ]) -> None : self . graph = graph def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue class ParentChildOracle ( Oracle ): \"\"\"Parent and children oracle for conditional independence testing. An oracle that knows the definite parents and children of every node. \"\"\" def get_children ( self , x ): \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x ) def get_parents ( self , x ): \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x ) class MarkovBlanketOracle ( ParentChildOracle ): \"\"\"MB oracle for conditional independence testing. An oracle that knows the definite Markov Blanket of every node. \"\"\" def __init__ ( self , graph : Union [ ADMG , nx . DiGraph ]) -> None : super () . __init__ ( graph ) def get_markov_blanket ( self , x ): \"\"\"Return the markov blanket of node 'x'.\"\"\" return self . graph . markov_blanket_of ( x ) class AncestralOracle ( ParentChildOracle ): \"\"\"Oracle with access to ancestors of any specific node.\"\"\" def get_ancestors ( self , x ): return self . graph . ancestors ( x ) Classes AncestralOracle class AncestralOracle ( graph : Union [ pywhy_graphs . admg . ADMG , networkx . classes . digraph . DiGraph ] ) View Source class AncestralOracle ( ParentChildOracle ): \"\"\"Oracle with access to ancestors of any specific node.\"\"\" def get_ancestors ( self , x ): return self . graph . ancestors ( x ) Ancestors (in MRO) dodiscover.ci.oracle.ParentChildOracle dodiscover.ci.oracle.Oracle dodiscover.ci.base.BaseConditionalIndependenceTest Methods get_ancestors def get_ancestors ( self , x ) View Source def get_ancestors ( self , x ) : return self . graph . ancestors ( x ) get_children def get_children ( self , x ) Return the definite children of node 'x'. View Source def get_children ( self , x ) : \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x ) get_parents def get_parents ( self , x ) Return the definite parents of node 'x'. View Source def get_parents ( self , x ) : \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x ) test def test ( self , df , x_var , y_var , z_covariates ) Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters: Name Type Description Default df pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. None x_var node A node in the dataset. None y_var node A node in the dataset. None z_covariates set The set of variables to check that separates x_var and y_var. None Returns: Type Description None A return argument for the statistic. View Source def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue MarkovBlanketOracle class MarkovBlanketOracle ( graph : Union [ pywhy_graphs . admg . ADMG , networkx . classes . digraph . DiGraph ] ) View Source class MarkovBlanketOracle ( ParentChildOracle ): \"\"\"MB oracle for conditional independence testing. An oracle that knows the definite Markov Blanket of every node. \"\"\" def __init__ ( self , graph: Union [ ADMG , nx . DiGraph ]) -> None: super (). __init__ ( graph ) def get_markov_blanket ( self , x ): \"\"\"Return the markov blanket of node 'x'.\"\"\" return self . graph . markov_blanket_of ( x ) Ancestors (in MRO) dodiscover.ci.oracle.ParentChildOracle dodiscover.ci.oracle.Oracle dodiscover.ci.base.BaseConditionalIndependenceTest Methods get_children def get_children ( self , x ) Return the definite children of node 'x'. View Source def get_children ( self , x ) : \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x ) get_markov_blanket def get_markov_blanket ( self , x ) Return the markov blanket of node 'x'. View Source def get_markov_blanket ( self , x ) : \"\"\"Return the markov blanket of node 'x'.\"\"\" return self . graph . markov_blanket_of ( x ) get_parents def get_parents ( self , x ) Return the definite parents of node 'x'. View Source def get_parents ( self , x ) : \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x ) test def test ( self , df , x_var , y_var , z_covariates ) Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters: Name Type Description Default df pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. None x_var node A node in the dataset. None y_var node A node in the dataset. None z_covariates set The set of variables to check that separates x_var and y_var. None Returns: Type Description None A return argument for the statistic. View Source def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue Oracle class Oracle ( graph : Union [ pywhy_graphs . admg . ADMG , networkx . classes . digraph . DiGraph ] ) Attributes Name Type Description Default graph nx.DiGraph pywhy_graphs.ADMG The ground-truth causal graph. View Source class Oracle ( BaseConditionalIndependenceTest ): \"\"\"Oracle conditional independence testing. Used for unit testing and checking intuition. Parameters ---------- graph : nx.DiGraph | pywhy_graphs.ADMG The ground-truth causal graph. \"\"\" def __init__ ( self , graph : Union [ ADMG , nx . DiGraph ]) -> None : self . graph = graph def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue Ancestors (in MRO) dodiscover.ci.base.BaseConditionalIndependenceTest Descendants dodiscover.ci.oracle.ParentChildOracle Methods test def test ( self , df , x_var , y_var , z_covariates ) Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters: Name Type Description Default df pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. None x_var node A node in the dataset. None y_var node A node in the dataset. None z_covariates set The set of variables to check that separates x_var and y_var. None Returns: Type Description None A return argument for the statistic. View Source def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue ParentChildOracle class ParentChildOracle ( graph : Union [ pywhy_graphs . admg . ADMG , networkx . classes . digraph . DiGraph ] ) View Source class ParentChildOracle ( Oracle ): \"\"\"Parent and children oracle for conditional independence testing. An oracle that knows the definite parents and children of every node. \"\"\" def get_children ( self , x ): \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x ) def get_parents ( self , x ): \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x ) Ancestors (in MRO) dodiscover.ci.oracle.Oracle dodiscover.ci.base.BaseConditionalIndependenceTest Descendants dodiscover.ci.oracle.MarkovBlanketOracle dodiscover.ci.oracle.AncestralOracle Methods get_children def get_children ( self , x ) Return the definite children of node 'x'. View Source def get_children ( self , x ) : \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x ) get_parents def get_parents ( self , x ) Return the definite parents of node 'x'. View Source def get_parents ( self , x ) : \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x ) test def test ( self , df , x_var , y_var , z_covariates ) Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters: Name Type Description Default df pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. None x_var node A node in the dataset. None y_var node A node in the dataset. None z_covariates set The set of variables to check that separates x_var and y_var. None Returns: Type Description None A return argument for the statistic. View Source def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue","title":"Oracle"},{"location":"reference/dodiscover/ci/oracle/#module-dodiscovercioracle","text":"None None View Source from typing import Union import networkx as nx import numpy as np from graphs.algorithms import m_separated from networkx.algorithms import d_separated from pywhy_graphs import ADMG from .base import BaseConditionalIndependenceTest class Oracle ( BaseConditionalIndependenceTest ): \"\"\"Oracle conditional independence testing. Used for unit testing and checking intuition. Parameters ---------- graph : nx.DiGraph | pywhy_graphs.ADMG The ground-truth causal graph. \"\"\" def __init__ ( self , graph : Union [ ADMG , nx . DiGraph ]) -> None : self . graph = graph def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue class ParentChildOracle ( Oracle ): \"\"\"Parent and children oracle for conditional independence testing. An oracle that knows the definite parents and children of every node. \"\"\" def get_children ( self , x ): \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x ) def get_parents ( self , x ): \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x ) class MarkovBlanketOracle ( ParentChildOracle ): \"\"\"MB oracle for conditional independence testing. An oracle that knows the definite Markov Blanket of every node. \"\"\" def __init__ ( self , graph : Union [ ADMG , nx . DiGraph ]) -> None : super () . __init__ ( graph ) def get_markov_blanket ( self , x ): \"\"\"Return the markov blanket of node 'x'.\"\"\" return self . graph . markov_blanket_of ( x ) class AncestralOracle ( ParentChildOracle ): \"\"\"Oracle with access to ancestors of any specific node.\"\"\" def get_ancestors ( self , x ): return self . graph . ancestors ( x )","title":"Module dodiscover.ci.oracle"},{"location":"reference/dodiscover/ci/oracle/#classes","text":"","title":"Classes"},{"location":"reference/dodiscover/ci/oracle/#ancestraloracle","text":"class AncestralOracle ( graph : Union [ pywhy_graphs . admg . ADMG , networkx . classes . digraph . DiGraph ] ) View Source class AncestralOracle ( ParentChildOracle ): \"\"\"Oracle with access to ancestors of any specific node.\"\"\" def get_ancestors ( self , x ): return self . graph . ancestors ( x )","title":"AncestralOracle"},{"location":"reference/dodiscover/ci/oracle/#ancestors-in-mro","text":"dodiscover.ci.oracle.ParentChildOracle dodiscover.ci.oracle.Oracle dodiscover.ci.base.BaseConditionalIndependenceTest","title":"Ancestors (in MRO)"},{"location":"reference/dodiscover/ci/oracle/#methods","text":"","title":"Methods"},{"location":"reference/dodiscover/ci/oracle/#get_ancestors","text":"def get_ancestors ( self , x ) View Source def get_ancestors ( self , x ) : return self . graph . ancestors ( x )","title":"get_ancestors"},{"location":"reference/dodiscover/ci/oracle/#get_children","text":"def get_children ( self , x ) Return the definite children of node 'x'. View Source def get_children ( self , x ) : \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x )","title":"get_children"},{"location":"reference/dodiscover/ci/oracle/#get_parents","text":"def get_parents ( self , x ) Return the definite parents of node 'x'. View Source def get_parents ( self , x ) : \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x )","title":"get_parents"},{"location":"reference/dodiscover/ci/oracle/#test","text":"def test ( self , df , x_var , y_var , z_covariates ) Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters: Name Type Description Default df pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. None x_var node A node in the dataset. None y_var node A node in the dataset. None z_covariates set The set of variables to check that separates x_var and y_var. None Returns: Type Description None A return argument for the statistic. View Source def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue","title":"test"},{"location":"reference/dodiscover/ci/oracle/#markovblanketoracle","text":"class MarkovBlanketOracle ( graph : Union [ pywhy_graphs . admg . ADMG , networkx . classes . digraph . DiGraph ] ) View Source class MarkovBlanketOracle ( ParentChildOracle ): \"\"\"MB oracle for conditional independence testing. An oracle that knows the definite Markov Blanket of every node. \"\"\" def __init__ ( self , graph: Union [ ADMG , nx . DiGraph ]) -> None: super (). __init__ ( graph ) def get_markov_blanket ( self , x ): \"\"\"Return the markov blanket of node 'x'.\"\"\" return self . graph . markov_blanket_of ( x )","title":"MarkovBlanketOracle"},{"location":"reference/dodiscover/ci/oracle/#ancestors-in-mro_1","text":"dodiscover.ci.oracle.ParentChildOracle dodiscover.ci.oracle.Oracle dodiscover.ci.base.BaseConditionalIndependenceTest","title":"Ancestors (in MRO)"},{"location":"reference/dodiscover/ci/oracle/#methods_1","text":"","title":"Methods"},{"location":"reference/dodiscover/ci/oracle/#get_children_1","text":"def get_children ( self , x ) Return the definite children of node 'x'. View Source def get_children ( self , x ) : \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x )","title":"get_children"},{"location":"reference/dodiscover/ci/oracle/#get_markov_blanket","text":"def get_markov_blanket ( self , x ) Return the markov blanket of node 'x'. View Source def get_markov_blanket ( self , x ) : \"\"\"Return the markov blanket of node 'x'.\"\"\" return self . graph . markov_blanket_of ( x )","title":"get_markov_blanket"},{"location":"reference/dodiscover/ci/oracle/#get_parents_1","text":"def get_parents ( self , x ) Return the definite parents of node 'x'. View Source def get_parents ( self , x ) : \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x )","title":"get_parents"},{"location":"reference/dodiscover/ci/oracle/#test_1","text":"def test ( self , df , x_var , y_var , z_covariates ) Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters: Name Type Description Default df pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. None x_var node A node in the dataset. None y_var node A node in the dataset. None z_covariates set The set of variables to check that separates x_var and y_var. None Returns: Type Description None A return argument for the statistic. View Source def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue","title":"test"},{"location":"reference/dodiscover/ci/oracle/#oracle","text":"class Oracle ( graph : Union [ pywhy_graphs . admg . ADMG , networkx . classes . digraph . DiGraph ] )","title":"Oracle"},{"location":"reference/dodiscover/ci/oracle/#attributes","text":"Name Type Description Default graph nx.DiGraph pywhy_graphs.ADMG The ground-truth causal graph. View Source class Oracle ( BaseConditionalIndependenceTest ): \"\"\"Oracle conditional independence testing. Used for unit testing and checking intuition. Parameters ---------- graph : nx.DiGraph | pywhy_graphs.ADMG The ground-truth causal graph. \"\"\" def __init__ ( self , graph : Union [ ADMG , nx . DiGraph ]) -> None : self . graph = graph def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue","title":"Attributes"},{"location":"reference/dodiscover/ci/oracle/#ancestors-in-mro_2","text":"dodiscover.ci.base.BaseConditionalIndependenceTest","title":"Ancestors (in MRO)"},{"location":"reference/dodiscover/ci/oracle/#descendants","text":"dodiscover.ci.oracle.ParentChildOracle","title":"Descendants"},{"location":"reference/dodiscover/ci/oracle/#methods_2","text":"","title":"Methods"},{"location":"reference/dodiscover/ci/oracle/#test_2","text":"def test ( self , df , x_var , y_var , z_covariates ) Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters: Name Type Description Default df pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. None x_var node A node in the dataset. None y_var node A node in the dataset. None z_covariates set The set of variables to check that separates x_var and y_var. None Returns: Type Description None A return argument for the statistic. View Source def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue","title":"test"},{"location":"reference/dodiscover/ci/oracle/#parentchildoracle","text":"class ParentChildOracle ( graph : Union [ pywhy_graphs . admg . ADMG , networkx . classes . digraph . DiGraph ] ) View Source class ParentChildOracle ( Oracle ): \"\"\"Parent and children oracle for conditional independence testing. An oracle that knows the definite parents and children of every node. \"\"\" def get_children ( self , x ): \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x ) def get_parents ( self , x ): \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x )","title":"ParentChildOracle"},{"location":"reference/dodiscover/ci/oracle/#ancestors-in-mro_3","text":"dodiscover.ci.oracle.Oracle dodiscover.ci.base.BaseConditionalIndependenceTest","title":"Ancestors (in MRO)"},{"location":"reference/dodiscover/ci/oracle/#descendants_1","text":"dodiscover.ci.oracle.MarkovBlanketOracle dodiscover.ci.oracle.AncestralOracle","title":"Descendants"},{"location":"reference/dodiscover/ci/oracle/#methods_3","text":"","title":"Methods"},{"location":"reference/dodiscover/ci/oracle/#get_children_2","text":"def get_children ( self , x ) Return the definite children of node 'x'. View Source def get_children ( self , x ) : \"\"\"Return the definite children of node 'x'.\"\"\" return self . graph . successors ( x )","title":"get_children"},{"location":"reference/dodiscover/ci/oracle/#get_parents_2","text":"def get_parents ( self , x ) Return the definite parents of node 'x'. View Source def get_parents ( self , x ) : \"\"\"Return the definite parents of node 'x'.\"\"\" return self . graph . predecessors ( x )","title":"get_parents"},{"location":"reference/dodiscover/ci/oracle/#test_3","text":"def test ( self , df , x_var , y_var , z_covariates ) Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters: Name Type Description Default df pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. None x_var node A node in the dataset. None y_var node A node in the dataset. None z_covariates set The set of variables to check that separates x_var and y_var. None Returns: Type Description None A return argument for the statistic. View Source def test ( self , df , x_var , y_var , z_covariates ): \"\"\"Conditional independence test given an oracle. Checks conditional independence between 'x_var' and 'y_var' given 'z_covariates' of variables using the causal graph as an oracle. Parameters ---------- df : pd.DataFrame of shape (n_samples, n_variables) The data matrix. Passed in for API consistency, but not used. x_var : node A node in the dataset. y_var : node A node in the dataset. z_covariates : set The set of variables to check that separates x_var and y_var. Returns ------- statistic : None A return argument for the statistic. pvalue : float The pvalue. Return '1.0' if not independent and '0.0' if they are. \"\"\" self . _check_test_input ( df , x_var , y_var , z_covariates ) # just check for d-separation between x and y # given sep_set if isinstance ( self . graph , nx . DiGraph ): is_sep = d_separated ( self . graph , { x_var }, { y_var }, z_covariates ) else : is_sep = m_separated ( self . graph , { x_var }, { y_var }, z_covariates ) if is_sep : pvalue = 1 test_stat = 0 else : pvalue = 0 test_stat = np . inf return test_stat , pvalue","title":"test"}]}